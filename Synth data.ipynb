{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca5cd0f1-590d-49ce-9086-7e7ad55f98ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df4bc2-2dc3-49e1-94c7-b40b90325a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from connect import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3607f-0c58-48da-9267-5b8785d7298b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb3b5f-dc7f-4df5-a382-4c6120ee58b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import NAR.utils as utils\n",
    "import NAR.data_management as dm\n",
    "import NAR.plotting_utils as pu\n",
    "import NAR.masking as masking\n",
    "import NAR.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058da714-78cb-42e6-9d69-e38f36ade4ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reload modules (DEV):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805529e4-73fb-4ac0-9124-d72b08aa7994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reload_modules([utils, dm, pu, masking, models])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711fdca5-ac7a-459a-a840-702f17a24f9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53edbcb8-eb6a-4046-b8ba-5d2b6082ed67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/home/jovyan/shared/jesus/datasets/synthetic/train_M_12_m_5_DIM_15_N_30000.csv')\n",
    "test_data = pd.read_csv('/home/jovyan/shared/jesus/datasets/synthetic/test_M_12_m_5_DIM_15_N_30000.csv')\n",
    "\n",
    "\n",
    "data = pd.concat([train_data, test_data])\n",
    "micro_classes = data['Micro Label'].unique()\n",
    "micro_classes.sort()\n",
    "macro_classes = data['Macro Label'].unique()\n",
    "macro_classes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b7521-960f-4f19-9833-0ca19ac8f299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Train Type A ZdAs: \\n', train_data[train_data.Type_A_ZdA]['Micro Label'].unique())\n",
    "print('Train Type B ZdAs: \\n', train_data[train_data.Type_B_ZdA]['Micro Label'].unique())\n",
    "print('Test Type A ZdAs: \\n', test_data[test_data.Type_A_ZdA]['Micro Label'].unique())\n",
    "print('Test Type B ZdAs: \\n', test_data[test_data.Type_B_ZdA]['Micro Label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9a7b3-ab0a-4133-8c27-2fc2bf626c31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b41cf-4e0e-4ff8-a915-ab78ef737cc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## helper code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8e43c-c1a1-4228-8d98-60bb6d378981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_stuff(prefix):\n",
    "    torch.save(\n",
    "        processor_1.state_dict(),\n",
    "        prefix+'_proc_1.pt')\n",
    "    torch.save(\n",
    "        processor_2.state_dict(),\n",
    "        prefix+'_proc_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc7dc5-62bb-4a98-80fc-a445b34a9c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def first_phase_simple(\n",
    "        sample_batch):\n",
    "\n",
    "    global cs_cm_1\n",
    "    global os_cm_1\n",
    "    global metrics_dict\n",
    "\n",
    "    # get masks: THESE ARE NOT COMPLEMETARY!\n",
    "    zda_mask, \\\n",
    "        known_classes_mask, \\\n",
    "        unknown_1_mask, \\\n",
    "        active_query_mask = utils.get_masks_1(\n",
    "            sample_batch[1],\n",
    "            N_QUERY,\n",
    "            device=device)\n",
    "\n",
    "    # get one_hot_labels:\n",
    "    oh_labels = utils.get_oh_labels(\n",
    "        decimal_labels=sample_batch[1][:, 1].long(),\n",
    "        total_classes=max_prototype_buffer_micro,\n",
    "        device=device)\n",
    "\n",
    "    # mask labels:\n",
    "    oh_masked_labels = utils.get_one_hot_masked_labels(\n",
    "        oh_labels,\n",
    "        unknown_1_mask,\n",
    "        device=device)\n",
    "\n",
    "    # encoding input space:\n",
    "    encoded_inputs = encoder(\n",
    "        sample_batch[0].float())\n",
    "\n",
    "    # processing\n",
    "    decoded_1, hiddens_1, predicted_kernel = processor_1(\n",
    "        encoded_inputs,\n",
    "        oh_masked_labels)\n",
    "\n",
    "    # semantic kernel:\n",
    "    semantic_kernel = oh_labels @ oh_labels.T\n",
    "    # Processor regularization:\n",
    "    proc_1_reg_loss = utils.get_kernel_kernel_loss(\n",
    "        semantic_kernel,\n",
    "        predicted_kernel,\n",
    "        a_w=attr_w,\n",
    "        r_w=rep_w)\n",
    "\n",
    "    # Transform labels for Few_shot Closed-set classif.\n",
    "    # compatible with the design of models.get_centroids functions,\n",
    "    # wich is called by our GAT processors.\n",
    "    unique_labels, transformed_labels = sample_batch[1][:, 1][active_query_mask].unique(\n",
    "        return_inverse=True)\n",
    "\n",
    "    # closed set classification\n",
    "    dec_1_loss_a = decoder_1a_criterion(\n",
    "        decoded_1[active_query_mask],\n",
    "        transformed_labels)\n",
    "\n",
    "    # Detach closed from open set gradients\n",
    "    input_for_os_dec = decoded_1.detach()\n",
    "    input_for_os_dec.requires_grad = True\n",
    "\n",
    "    # Unknown cluster prediction:\n",
    "    predicted_unknown_1s = decoder_1_b(\n",
    "        scores=input_for_os_dec[unknown_1_mask]\n",
    "        )\n",
    "\n",
    "    # open-set loss:\n",
    "    dec_1_loss_b = decoder_1b_criterion(\n",
    "        predicted_unknown_1s,\n",
    "        zda_mask[unknown_1_mask].float().unsqueeze(-1))\n",
    "\n",
    "    # inverse transform cs preds\n",
    "    it_preds = utils.inverse_transform_preds(\n",
    "        transormed_preds=decoded_1[active_query_mask],\n",
    "        real_labels=unique_labels,\n",
    "        real_class_num=max_prototype_buffer_micro)\n",
    "\n",
    "    #\n",
    "    # REPORTING:\n",
    "    #\n",
    "\n",
    "    # Closed set confusion matrix\n",
    "    cs_cm_1 += utils.efficient_cm(\n",
    "        preds=it_preds.detach(),\n",
    "        targets=sample_batch[1][:, 1][active_query_mask].long())\n",
    "\n",
    "    # Open set confusion matrix\n",
    "    os_cm_1 += utils.efficient_os_cm(\n",
    "        preds=(predicted_unknown_1s.detach() > 0.5).long(),\n",
    "        targets=zda_mask[unknown_1_mask].long()\n",
    "        )\n",
    "\n",
    "    # accuracies:\n",
    "    CS_acc = utils.get_acc(\n",
    "        logits_preds=it_preds,\n",
    "        oh_labels=oh_labels[active_query_mask])\n",
    "\n",
    "    OS_acc = utils.get_binary_acc(\n",
    "        logits=predicted_unknown_1s.detach(),\n",
    "        labels=zda_mask[unknown_1_mask].float().unsqueeze(-1))\n",
    "\n",
    "    OS_b_acc = utils.get_balanced_accuracy(\n",
    "                os_cm=os_cm_1,\n",
    "                n_w=balanced_acc_n_w\n",
    "                )\n",
    "\n",
    "    # for reporting:\n",
    "    metrics_dict['losses_1a'].append(dec_1_loss_a.item())\n",
    "    metrics_dict['proc_reg_loss1'].append(proc_1_reg_loss.item())\n",
    "    metrics_dict['CS_accuracies'].append(CS_acc.item())\n",
    "    metrics_dict['losses_1b'].append(dec_1_loss_b.item())\n",
    "    metrics_dict['OS_accuracies'].append(OS_acc.item())\n",
    "    metrics_dict['OS_B_accuracies'].append(OS_b_acc.item())\n",
    "\n",
    "    # Processor loss:\n",
    "    proc_1_loss = dec_1_loss_a + proc_1_reg_loss\n",
    "\n",
    "    return proc_1_loss, \\\n",
    "        dec_1_loss_b, \\\n",
    "        hiddens_1, \\\n",
    "        decoded_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c20b787-a369-46ea-b63e-ce1724ff1a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def second_phase_simple(\n",
    "        sample_batch,\n",
    "        hiddens_1):\n",
    "\n",
    "    global cs_cm_2\n",
    "    global os_cm_2\n",
    "    global metrics_dict\n",
    "\n",
    "    # get masks: THESE ARE NOT COMPLEMETARY!\n",
    "    type_A_mask, known_macro_classes_mask, \\\n",
    "        unknown_2_mask, active_query_mask_2 = utils.get_masks_2(\n",
    "            sample_batch[1],\n",
    "            N_QUERY,\n",
    "            device=device)\n",
    "\n",
    "    # get one_hot_labels:\n",
    "    oh_labels = utils.get_oh_labels(\n",
    "        decimal_labels=sample_batch[1][:, 0].long(),\n",
    "        total_classes=max_prototype_buffer_macro,\n",
    "        device=device)\n",
    "\n",
    "    # mask labels:\n",
    "    oh_masked_labels = utils.get_one_hot_masked_labels(\n",
    "        oh_labels,\n",
    "        unknown_2_mask,\n",
    "        device=device)\n",
    "\n",
    "    decoded_2, hiddens_2, predicted_kernel_2 = processor_2(\n",
    "        hiddens_1,\n",
    "        oh_masked_labels)\n",
    "\n",
    "    # semantic kernel:\n",
    "    semantic_kernel_2 = oh_labels @ oh_labels.T\n",
    "    # Processor regularization:\n",
    "    proc_2_reg_loss = utils.get_kernel_kernel_loss(\n",
    "        semantic_kernel_2,\n",
    "        predicted_kernel_2,\n",
    "        a_w=attr_w,\n",
    "        r_w=rep_w)\n",
    "\n",
    "    unique_macro_labels, transformed_labels_2 = sample_batch[1][:, 0][active_query_mask_2].unique(\n",
    "        return_inverse=True)\n",
    "\n",
    "    # Closed set: should learn to associate type B's to corr. macro cluster.\n",
    "    # geometrical \"break\" in the real-data case. (GRadients 2A)\n",
    "    dec_2_loss_a = decoder_2a_criterion(\n",
    "        decoded_2[active_query_mask_2],\n",
    "        transformed_labels_2)\n",
    "\n",
    "    input_for_os_dec_2 = decoded_2.detach()\n",
    "    input_for_os_dec_2.requires_grad = True\n",
    "\n",
    "    # Unknown cluster prediction:\n",
    "    predicted_unknown_2s = decoder_2_b(\n",
    "        scores=input_for_os_dec_2[unknown_2_mask]\n",
    "        )\n",
    "\n",
    "    # open-set loss:\n",
    "    dec_2_loss_b = decoder_2b_criterion(\n",
    "        predicted_unknown_2s,\n",
    "        type_A_mask[unknown_2_mask].float().unsqueeze(-1))\n",
    "\n",
    "    # inverse transform cs preds\n",
    "    it_preds = utils.inverse_transform_preds(\n",
    "        transormed_preds=decoded_2[active_query_mask_2],\n",
    "        real_labels=unique_macro_labels,\n",
    "        real_class_num=max_prototype_buffer_macro)\n",
    "\n",
    "    # Closed set confusion matrix\n",
    "    cs_cm_2 += utils.efficient_cm(\n",
    "        preds=it_preds.detach(),\n",
    "        targets=sample_batch[1][:, 0][active_query_mask_2].long(),\n",
    "        )\n",
    "\n",
    "    # Open set confusion matrix\n",
    "    os_cm_2 += utils.efficient_os_cm(\n",
    "        preds=(predicted_unknown_2s.detach() > 0.5).long(),\n",
    "        targets=type_A_mask[unknown_2_mask].long()\n",
    "        )\n",
    "\n",
    "    # accuracies:\n",
    "    CS_acc_2 = utils.get_acc(\n",
    "        logits_preds=it_preds,\n",
    "        oh_labels=oh_labels[active_query_mask_2])\n",
    "\n",
    "    OS_acc_2 = utils.get_binary_acc(\n",
    "        logits=predicted_unknown_2s.detach(),\n",
    "        labels=type_A_mask[unknown_2_mask].float().unsqueeze(-1))\n",
    "\n",
    "    OS_2_B_acc = utils.get_balanced_accuracy(\n",
    "                os_cm=os_cm_2,\n",
    "                n_w=balanced_acc_n_w\n",
    "                )\n",
    "\n",
    "    proc_2_loss = dec_2_loss_a + proc_2_reg_loss\n",
    "\n",
    "    # for reporting:\n",
    "    metrics_dict['losses_2a'].append(dec_2_loss_a.item())\n",
    "    metrics_dict['proc_reg_loss2'].append(proc_2_reg_loss.item())\n",
    "    metrics_dict['losses_2b'].append(dec_2_loss_b.item())\n",
    "    metrics_dict['CS_2_accuracies'].append(CS_acc_2.item())\n",
    "    metrics_dict['OS_2_accuracies'].append(OS_acc_2.item())\n",
    "    metrics_dict['OS_2_B_accuracies'].append(OS_2_B_acc.item())\n",
    "\n",
    "    return proc_2_loss, \\\n",
    "        dec_2_loss_b, \\\n",
    "        hiddens_2, \\\n",
    "        decoded_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d118e201-21e9-454c-9837-93f705a9195c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## init data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d63b9-982b-4e04-ad2b-bb0d8271a971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "natural_inputs_dim = 15\n",
    "save = True\n",
    "wb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab974a58-3d4f-4ed8-90ef-433225757430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "torch_seed = 1234\n",
    "torch.manual_seed(torch_seed)\n",
    "noise = 10\n",
    "\n",
    "new_train_data = train_data.drop(columns=[\n",
    "    'Micro Label',\n",
    "    'Macro Label',\n",
    "    'ZdA',\n",
    "    'Type_A_ZdA',\n",
    "    'Type_B_ZdA']).values[:, :natural_inputs_dim]\n",
    "\n",
    "new_test_data = test_data.drop(columns=[\n",
    "    'Micro Label',\n",
    "    'Macro Label',\n",
    "    'ZdA',\n",
    "    'Type_A_ZdA',\n",
    "    'Type_B_ZdA']).values[:, :natural_inputs_dim]\n",
    "\n",
    "# NEW NOISE SCHEME:\n",
    "\n",
    "# Simulating different noise in different samples :)\n",
    "train_samples = new_train_data.shape[0]\n",
    "test_samples = new_test_data.shape[0]\n",
    "\n",
    "# Distribution of noise among features:\n",
    "noise_tensor = torch.rand(1, natural_inputs_dim) * noise\n",
    "noise_train = noise_tensor.repeat(train_samples, 1)\n",
    "noise_test = noise_tensor.repeat(test_samples, 1)\n",
    "\n",
    "new_train_data = new_train_data + noise_train.numpy()\n",
    "new_test_data = new_test_data + noise_test.numpy()\n",
    "\n",
    "#\n",
    "#\n",
    "# Initialize\n",
    "#\n",
    "#\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dataset and Dataloader:\n",
    "train_dataset = dm.SynthFewShotDataset(\n",
    "    features=new_train_data,\n",
    "    df=train_data)\n",
    "\n",
    "test_dataset = dm.SynthFewShotDataset(\n",
    "    features=new_test_data,\n",
    "    df=test_data)\n",
    "\n",
    "# Number of classes per task :\n",
    "# two of them are ZdAs, one is a type B and the other a type A\n",
    "N_WAY = 5\n",
    "N_SHOT = 5   # Number of samples per class in the support set\n",
    "N_QUERY = 20  # Number of samples per class in the query set\n",
    "\n",
    "n_train_tasks = 500    # For speedy tests, reduce here..\n",
    "n_eval_tasks = 100     # For speedy tests, reduce here..\n",
    "\n",
    "\n",
    "num_of_test_classes = test_dataset.micro_classes\n",
    "num_of_train_classes = train_dataset.micro_classes\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    sampler=dm.FewShotSampler(\n",
    "                dataset=train_dataset,\n",
    "                n_tasks=n_train_tasks,\n",
    "                classes_per_it=N_WAY,\n",
    "                k_shot=N_SHOT,\n",
    "                q_shot=N_QUERY),\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    collate_fn=dm.convenient_cf)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    sampler=dm.FewShotSampler(\n",
    "                dataset=test_dataset,\n",
    "                n_tasks=n_eval_tasks,\n",
    "                classes_per_it=N_WAY,\n",
    "                k_shot=N_SHOT,\n",
    "                q_shot=N_QUERY),\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    collate_fn=dm.convenient_cf)\n",
    "\n",
    "# reproducibility\n",
    "train_loader.sampler.reset()\n",
    "test_loader.sampler.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26568a9a-b487-4976-aa3c-332de63fd8e3",
   "metadata": {},
   "source": [
    "## Init Architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd612f-906b-4168-9bb8-bddad2a06cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Hyper params:\n",
    "###\n",
    "max_prototype_buffer_micro = 60\n",
    "max_prototype_buffer_macro = 12\n",
    "lr = 0.001\n",
    "# training parameters\n",
    "n_epochs = 60\n",
    "norm = \"batch\"\n",
    "dropout = 0.1\n",
    "patience = 8\n",
    "lambda_os = 1\n",
    "\n",
    "processor_attention_heads = 8\n",
    "h_dim = 1024\n",
    "report_step_frequency = 100\n",
    "colaborative_GAT = \"True by def\"\n",
    "pos_weight_1 = 5/2\n",
    "pos_weight_2 = 5\n",
    "architectures = 'GATV5 Confidence Dec'\n",
    "balanced_acc_n_w = 0.5\n",
    "attr_w = 1\n",
    "rep_w = 1\n",
    "\n",
    "run_name = f'Pretraining'\n",
    "\n",
    "if wb:\n",
    "    wandb.init(project='Nero_1.1',\n",
    "               name=run_name,\n",
    "               config={\"N_SHOT\": N_SHOT,\n",
    "                       \"N_QUERY\": N_QUERY,\n",
    "                       \"N_WAY\": N_WAY,\n",
    "                       \"num_of_test_classes\": num_of_test_classes,\n",
    "                       \"num_of_train_classes\": num_of_train_classes,\n",
    "                       \"train_batch_size\": iter(train_loader).__next__()[0].shape[1],\n",
    "                       \"len(train_loader)\": len(train_loader),\n",
    "                       \"len(test_dataset)\": len(test_dataset),\n",
    "                       \"max_prototype_buffer_micro\": max_prototype_buffer_micro,\n",
    "                       \"max_prototype_buffer_macro\": max_prototype_buffer_macro,\n",
    "                       \"device\": device,\n",
    "                       \"natural_inputs_dim\": natural_inputs_dim,\n",
    "                       \"h_dim\": h_dim,\n",
    "                       \"lr\": lr,\n",
    "                       \"n_epochs\": n_epochs,\n",
    "                       \"norm\": norm,\n",
    "                       \"dropout\": dropout,\n",
    "                       \"patience\": patience,\n",
    "                       'micro_zdas': data[data.ZdA == True]['Micro Label'].unique(),\n",
    "                       'micro_type_A_ZdAs': data[data.Type_A_ZdA == True]['Micro Label'].unique(),\n",
    "                       'micro_type_B_ZdAs': data[data.Type_B_ZdA == True]['Micro Label'].unique(),\n",
    "                       \"lambda_os\": lambda_os,\n",
    "                       \"colaborative_GAT\": colaborative_GAT,\n",
    "                       \"positive_weight_1\": pos_weight_1,\n",
    "                       \"positive_weight_2\": pos_weight_2,\n",
    "                       \"architectures\": architectures,\n",
    "                       \"balanced_acc_n_w\": balanced_acc_n_w,\n",
    "                       \"attr_w\": attr_w,\n",
    "                       \"rep_w\": rep_w\n",
    "                       })\n",
    "else:\n",
    "    print(run_name)\n",
    "\n",
    "# Encoder\n",
    "encoder = models.Encoder(\n",
    "    in_features=natural_inputs_dim,\n",
    "    out_features=h_dim,\n",
    "    norm=norm,\n",
    "    dropout=dropout,\n",
    "    ).to(device)\n",
    "\n",
    "# First phase:\n",
    "processor_1 = models.GAT_V5_Processor(\n",
    "                h_dim=h_dim,\n",
    "                processor_attention_heads=processor_attention_heads,\n",
    "                dropout=dropout,\n",
    "                device=device\n",
    "                ).to(device)\n",
    "\n",
    "decoder_1a_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "decoder_1_b = models.Confidence_Decoder(\n",
    "                in_dim=N_WAY-2, # Subtrack a type A and a type B ZdA attack\n",
    "                dropout=dropout,\n",
    "                device=device\n",
    "                ).to(device)\n",
    "\n",
    "decoder_1b_criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.Tensor([pos_weight_1])).to(device)\n",
    "\n",
    "# Second phase:\n",
    "processor_2 = models.GAT_V5_Processor(\n",
    "                h_dim=h_dim,\n",
    "                processor_attention_heads=processor_attention_heads,\n",
    "                dropout=dropout,\n",
    "                device=device\n",
    "                ).to(device)\n",
    "\n",
    "decoder_2a_criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "decoder_2_b = models.Confidence_Decoder(\n",
    "                in_dim=N_WAY-1, # Only type A attack will be discarded from known realm\n",
    "                dropout=dropout,\n",
    "                device=device\n",
    "                ).to(device)\n",
    "\n",
    "decoder_2b_criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.Tensor([pos_weight_2])).to(device)\n",
    "\n",
    "\n",
    "params_for_processor_optimizer = \\\n",
    "        list(encoder.parameters()) + \\\n",
    "        list(processor_1.parameters()) + \\\n",
    "        list(processor_2.parameters())\n",
    "\n",
    "processor_optimizer = optim.Adam(\n",
    "    params_for_processor_optimizer,\n",
    "    lr=lr)\n",
    "\n",
    "params_for_os_optimizer = \\\n",
    "        list(decoder_1_b.parameters()) + \\\n",
    "        list(decoder_2_b.parameters())\n",
    "\n",
    "os_optimizer = optim.Adam(\n",
    "    params_for_os_optimizer,\n",
    "    lr=lr)\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "max_eval_TNR = torch.zeros(1, device=device)\n",
    "epochs_without_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52732bd3-fde9-43ef-a087-fd4ddb11527d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.watch(processor_1)\n",
    "wandb.watch(processor_2)\n",
    "wandb.watch(encoder)\n",
    "wandb.watch(decoder_1_b)\n",
    "wandb.watch(decoder_2_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3eeb4-edb8-499a-a8ad-c24bf70f417e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0b122-cf1c-4379-b336-11d9da649979",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    # TRAIN\n",
    "    encoder.train()\n",
    "    processor_1.train()\n",
    "    decoder_1_b.train()\n",
    "    processor_2.train()\n",
    "    decoder_2_b.train()\n",
    "\n",
    "    # reset conf Mats\n",
    "    cs_cm_1 = torch.zeros(\n",
    "        [max_prototype_buffer_micro, max_prototype_buffer_micro],\n",
    "        device=device)\n",
    "    os_cm_1 = torch.zeros([2, 2], device=device)\n",
    "    cs_cm_2 = torch.zeros(\n",
    "        [max_prototype_buffer_macro, max_prototype_buffer_macro],\n",
    "        device=device)\n",
    "    os_cm_2 = torch.zeros([2, 2], device=device)\n",
    "\n",
    "    # reset metrics dict\n",
    "    metrics_dict = utils.reset_metrics_dict()\n",
    "\n",
    "    # go!\n",
    "    for batch_idx, sample_batch in enumerate(train_loader):\n",
    "        # go to cuda:\n",
    "        sample_batch = sample_batch[0].to(device), sample_batch[1].to(device)\n",
    "\n",
    "        # PHASE 1\n",
    "        proc_1_loss, \\\n",
    "            os_1_loss, \\\n",
    "            hiddens_1, \\\n",
    "            decoded_1 = first_phase_simple(\n",
    "                                sample_batch)\n",
    "\n",
    "        # PHASE 2\n",
    "        proc_2_loss, \\\n",
    "            os_2_loss, \\\n",
    "            hiddens_2, \\\n",
    "            decoded_2 = second_phase_simple(\n",
    "                sample_batch,\n",
    "                hiddens_1)\n",
    "\n",
    "        # Learning\n",
    "        proc_loss = proc_1_loss + proc_2_loss\n",
    "        processor_optimizer.zero_grad()\n",
    "        proc_loss.backward()\n",
    "        processor_optimizer.step()\n",
    "\n",
    "        os_loss = os_1_loss + os_2_loss\n",
    "        os_optimizer.zero_grad()\n",
    "        os_loss.backward()\n",
    "        os_optimizer.step()\n",
    "\n",
    "        # Reporting\n",
    "        step = batch_idx + (epoch * train_loader.sampler.n_tasks)\n",
    "\n",
    "        if step % report_step_frequency == 0:\n",
    "            utils.reporting_simple(\n",
    "                'train',\n",
    "                epoch,\n",
    "                metrics_dict,\n",
    "                step,\n",
    "                wb,\n",
    "                wandb)\n",
    "\n",
    "    pu.super_plotting_function(\n",
    "                phase='Training',\n",
    "                labels=sample_batch[1].cpu(),\n",
    "                hiddens_1=hiddens_1.detach().cpu(),\n",
    "                hiddens_2=hiddens_2.detach().cpu(),\n",
    "                scores_1=decoded_1.detach().cpu(),\n",
    "                scores_2=decoded_2.detach().cpu(),\n",
    "                cs_cm_1=cs_cm_1.cpu(),\n",
    "                cs_cm_2=cs_cm_2.cpu(),\n",
    "                os_cm_1=os_cm_1.cpu(),\n",
    "                os_cm_2=os_cm_2.cpu(),\n",
    "                wb=wb,\n",
    "                wandb=wandb,\n",
    "                complete_micro_classes=micro_classes,\n",
    "                complete_macro_classes=macro_classes\n",
    "                )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        # Evaluation\n",
    "        encoder.eval()\n",
    "        processor_1.eval()\n",
    "        decoder_1_b.eval()\n",
    "        processor_2.eval()\n",
    "        decoder_2_b.eval()\n",
    "\n",
    "        # reset conf Mats\n",
    "        cs_cm_1 = torch.zeros(\n",
    "            [max_prototype_buffer_micro, max_prototype_buffer_micro],\n",
    "            device=device)\n",
    "        os_cm_1 = torch.zeros([2, 2], device=device)\n",
    "        cs_cm_2 = torch.zeros(\n",
    "            [max_prototype_buffer_macro, max_prototype_buffer_macro],\n",
    "            device=device)\n",
    "        os_cm_2 = torch.zeros([2, 2], device=device)\n",
    "\n",
    "        # reset metrics dict\n",
    "        metrics_dict = utils.reset_metrics_dict()\n",
    "\n",
    "        # go!\n",
    "        for batch_idx, sample_batch in enumerate(test_loader):\n",
    "            # go to cuda:\n",
    "            sample_batch = sample_batch[0].to(device), sample_batch[1].to(device)\n",
    "\n",
    "            # PHASE 1\n",
    "            proc_1_loss, \\\n",
    "                os_1_loss, \\\n",
    "                hiddens_1, \\\n",
    "                decoded_1 = first_phase_simple(\n",
    "                                    sample_batch)\n",
    "\n",
    "            # PHASE 2\n",
    "            proc_2_loss, \\\n",
    "                os_2_loss, \\\n",
    "                hiddens_2, \\\n",
    "                decoded_2 = second_phase_simple(\n",
    "                    sample_batch,\n",
    "                    hiddens_1)\n",
    "\n",
    "            # Reporting\n",
    "            step = batch_idx + (epoch * test_loader.sampler.n_tasks)\n",
    "\n",
    "            if step % report_step_frequency == 0:\n",
    "                utils.reporting_simple(\n",
    "                        'eval',\n",
    "                        epoch,\n",
    "                        metrics_dict,\n",
    "                        step,\n",
    "                        wb,\n",
    "                        wandb)\n",
    "\n",
    "        pu.super_plotting_function(\n",
    "                phase='Evaluation',\n",
    "                labels=sample_batch[1].cpu(),\n",
    "                hiddens_1=hiddens_1.detach().cpu(),\n",
    "                hiddens_2=hiddens_2.detach().cpu(),\n",
    "                scores_1=decoded_1.detach().cpu(),\n",
    "                scores_2=decoded_2.detach().cpu(),\n",
    "                cs_cm_1=cs_cm_1.cpu(),\n",
    "                cs_cm_2=cs_cm_2.cpu(),\n",
    "                os_cm_1=os_cm_1.cpu(),\n",
    "                os_cm_2=os_cm_2.cpu(),\n",
    "                wb=wb,\n",
    "                wandb=wandb,\n",
    "                complete_micro_classes=micro_classes,\n",
    "                complete_macro_classes=macro_classes\n",
    "            )\n",
    "\n",
    "        # Checking for improvement\n",
    "        curr_TNR = np.array(metrics_dict['OS_2_B_accuracies']).mean()\n",
    "\n",
    "        if curr_TNR > max_eval_TNR:\n",
    "            max_eval_TNR = curr_TNR\n",
    "            epochs_without_improvement = 0\n",
    "            save_stuff(run_name)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping at step {step}')\n",
    "            if wb:\n",
    "                wandb.log({'Early stopping at episode': step})\n",
    "            break\n",
    "\n",
    "print(f'max_eval_TNR: {max_eval_TNR}')\n",
    "\n",
    "if wb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9831d3-5748-4ac4-9240-2079d5b83c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598d9e6-5b34-4450-be14-397980f8ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wanna plot something?\n",
    "train_data[['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14']] = new_train_data\n",
    "pu.plot_masked_dataset(train_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
