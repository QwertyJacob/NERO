{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca5cd0f1-590d-49ce-9086-7e7ad55f98ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df4bc2-2dc3-49e1-94c7-b40b90325a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from connect import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3607f-0c58-48da-9267-5b8785d7298b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb3b5f-dc7f-4df5-a382-4c6120ee58b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import NAR.utils as utils\n",
    "import NAR.data_management as dm\n",
    "import NAR.plotting_utils as pu\n",
    "import NAR.masking as masking\n",
    "import NAR.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058da714-78cb-42e6-9d69-e38f36ade4ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reload modules (DEV):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805529e4-73fb-4ac0-9124-d72b08aa7994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reload_modules([utils, dm, pu, masking, models])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711fdca5-ac7a-459a-a840-702f17a24f9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Checkpoint:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a435c133-8228-452c-9675-a163ff954463",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Edge to IIoT set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a91ea2-919c-4ed0-90ea-553941f5bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'data/pre_processed_iiotset.csv',\n",
    "    low_memory=False)\n",
    "df = df[df['Attack_type'] != 'MITM_-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ee6b0-320a-4825-ba67-7844987e0f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop(['Attack_label', 'attack_macro_cat', 'Attack_type'], axis=1)\n",
    "y = df[['attack_macro_cat', 'Attack_type']]\n",
    "\n",
    "categorical_indicator = X.dtypes == 'O'\n",
    "\n",
    "categorical_columns = X.columns[list(np.where(np.array(categorical_indicator)==True)[0])].tolist()\n",
    "cont_columns = list(set(X.columns.tolist()) - set(categorical_columns))\n",
    "\n",
    "cat_idxs = list(np.where(np.array(categorical_indicator)==True)[0])\n",
    "con_idxs = list(set(range(len(X.columns))) - set(cat_idxs))\n",
    "\n",
    "# Categories and target classes to natural numbers:\n",
    "cat_dims = []\n",
    "for col in categorical_columns:\n",
    "    l_enc = LabelEncoder()\n",
    "    X[col] = l_enc.fit_transform(X[col].values)\n",
    "    cat_dims.append(len(l_enc.classes_))\n",
    "\n",
    "X = X.values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "y = y.values\n",
    "macro_l_enc = LabelEncoder()\n",
    "micro_l_enc = LabelEncoder()\n",
    "macro_y = macro_l_enc.fit_transform(y[:, 0])\n",
    "micro_y = micro_l_enc.fit_transform(y[:, 1])\n",
    "\n",
    "# num of classes:\n",
    "num_macro_classes = len(np.unique(macro_y))\n",
    "num_micro_classes = len(np.unique(micro_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b8a3e7-01f3-44b3-802f-f9fdca7f2a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.groupby(['attack_macro_cat', 'Attack_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb49f2-ac00-45fa-8dc8-d9b4dff99f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting to suitable format:\n",
    "data = pd.DataFrame(X)\n",
    "data['Macro Label'] = macro_l_enc.inverse_transform(macro_y)\n",
    "data['Micro Label'] = micro_l_enc.inverse_transform(micro_y)\n",
    "\n",
    "\n",
    "micro_zdas = [\n",
    "        'MITM_0',                       # Type A\n",
    "        'MITM_1',                       # Type A\n",
    "        'MITM_2',                       # Type A\n",
    "        'MITM_3',                       # Type A\n",
    "        'Fingerprinting',              # Type A\n",
    "        'Port_Scanning',               # Type A\n",
    "        'Vulnerability_scanner',       # Type A\n",
    "        'Backdoor',                    # Type B\n",
    "        'DDoS_ICMP',                   # Type B\n",
    "        'DDoS_HTTP',                   # Type B\n",
    "        'SQL_injection',               # Type B\n",
    "        'Uploading',                   # Type B\n",
    "        'Password'                     # Type B\n",
    "        ]\n",
    "\n",
    "micro_type_A_ZdAs = [\n",
    "        'MITM_0',                       # Type A\n",
    "        'MITM_1',                       # Type A\n",
    "        'MITM_2',                       # Type A\n",
    "        'MITM_3',                       # Type A\n",
    "        'Fingerprinting',              # Type A\n",
    "        'Port_Scanning',               # Type A\n",
    "        'Vulnerability_scanner',       # Type A\n",
    "        ]\n",
    "\n",
    "micro_type_B_ZdAs = [\n",
    "        'Backdoor',                    # Type B\n",
    "        'DDoS_ICMP',                   # Type B\n",
    "        'DDoS_HTTP',                   # Type B\n",
    "        'SQL_injection',               # Type B\n",
    "        'Uploading',                   # Type B\n",
    "        'Password'                     # Type B\n",
    "        ]\n",
    "\n",
    "train_type_B_micro_classes = [\n",
    "        'Backdoor',                    # Type B\n",
    "        'DDoS_ICMP',                   # Type B\n",
    "        'Uploading'                    # Type B\n",
    "        ]\n",
    "\n",
    "test_type_B_micro_classes = [\n",
    "        'DDoS_HTTP',                   # Type B\n",
    "        'SQL_injection',               # Type B\n",
    "        'Password'                     # Type B\n",
    "        ]\n",
    "\n",
    "\n",
    "test_type_A_macro_classes = [\n",
    "        'MITM'                         # Type A\n",
    "        ]\n",
    "\n",
    "train_type_A_macro_classes = [\n",
    "        'Information_Gathering'        # Type A\n",
    "        ]\n",
    "\n",
    "\n",
    "data = masking.mask_real_data_lowdim(\n",
    "    data=data,\n",
    "    micro_zdas=micro_zdas,\n",
    "    micro_type_A_ZdAs=micro_type_A_ZdAs,\n",
    "    micro_type_B_ZdAs=micro_type_B_ZdAs\n",
    "    )\n",
    "\n",
    "train_data, test_data = masking.split_real_data(\n",
    "    data,\n",
    "    train_type_B_micro_classes,\n",
    "    test_type_B_micro_classes,\n",
    "    test_type_A_macro_classes,\n",
    "    train_type_A_macro_classes\n",
    "    )\n",
    "\n",
    "micro_classes = data['Micro Label'].unique()\n",
    "macro_classes = data['Macro Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a67a36-ecbe-4700-b19a-1043fdccb25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train test split is psuedo-randomic. for this reason we do it just once.\n",
    "train_data.to_csv('data/iiotset_train.csv', index=0)\n",
    "test_data.to_csv('data/iiotset_test.csv', index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87511a44-76b9-40fe-9278-3486ce48008f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Checkpoint 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d32829-cf9c-47a5-b949-39d76892ef4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/iiotset_train.csv', low_memory=False)\n",
    "test_data = pd.read_csv('data/iiotset_test.csv', low_memory=False)\n",
    "\n",
    "data = pd.concat([train_data, test_data])\n",
    "\n",
    "micro_classes = data['Micro Label'].unique()\n",
    "macro_classes = data['Macro Label'].unique()\n",
    "\n",
    "with open('data/micro_label_encoder.pkl', 'rb') as file:\n",
    "    micro_label_encoder = pickle.load(file)\n",
    "\n",
    "with open('data/macro_label_encoder.pkl', 'rb') as file:\n",
    "    macro_label_encoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9a7b3-ab0a-4133-8c27-2fc2bf626c31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b41cf-4e0e-4ff8-a915-ab78ef737cc3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## helper code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfcd28c-7c6a-4351-92ea-523886754da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_stuff(prefix):\n",
    "    torch.save(\n",
    "        micro_classifier.state_dict(),\n",
    "        prefix+'_micro_classifier.pt')\n",
    "    torch.save(\n",
    "        macro_classifier.state_dict(),\n",
    "        prefix+'_macro_classifier.pt')\n",
    "    torch.save(\n",
    "        micro_anomaly_detector.state_dict(),\n",
    "        prefix+'_micro_anomaly_detector.pt')\n",
    "    torch.save(\n",
    "        macro_anomaly_detector.state_dict(),\n",
    "        prefix+'_macro_anomaly_detector.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be584813-6ea9-4f99-a139-daf1696cc524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_classif(\n",
    "        sample_batch,\n",
    "        batch_idx,\n",
    "        closed_set=True,\n",
    "        open_set=True):\n",
    "\n",
    "    global cs_cm_1\n",
    "    global os_cm_1\n",
    "    global metrics_dict\n",
    "\n",
    "    # get masks: THESE ARE NOT COMPLEMETARY!\n",
    "    zda_mask, \\\n",
    "        known_classes_mask, \\\n",
    "        unknown_1_mask, \\\n",
    "        active_query_mask = utils.get_masks_1(\n",
    "            sample_batch[1],\n",
    "            N_QUERY,\n",
    "            device=device)\n",
    "\n",
    "    dec_1_loss_b = None\n",
    "    micro_loss = None\n",
    "    \n",
    "    # forward passes\n",
    "    micro_logits = micro_classifier(sample_batch[0])\n",
    "    \n",
    "    # get one_hot_labels:\n",
    "    oh_labels = utils.get_oh_labels(\n",
    "        decimal_labels=sample_batch[1][:, 1].long(),\n",
    "        total_classes=max_prototype_buffer_micro,\n",
    "        device=device)\n",
    "    \n",
    "    # Closed set confusion matrix\n",
    "    cs_cm_1 = cs_cm_1 + utils.efficient_cm(\n",
    "        preds=micro_logits[active_query_mask].detach(),\n",
    "        targets=sample_batch[1][:, 1][active_query_mask].long())\n",
    "\n",
    "    # accuracies:\n",
    "    CS_acc = utils.get_acc(\n",
    "        logits_preds=micro_logits[active_query_mask].detach(),\n",
    "        oh_labels=oh_labels[active_query_mask])\n",
    "\n",
    "    metrics_dict['CS_accuracies'][batch_idx] = CS_acc.detach()\n",
    "\n",
    "    if closed_set:\n",
    "\n",
    "        # loss computation\n",
    "        micro_loss = micro_multiclass_error(\n",
    "            micro_logits[active_query_mask],\n",
    "            sample_batch[1][:, 1][active_query_mask].long())        \n",
    "\n",
    "        # for reporting:\n",
    "        metrics_dict['losses_1a'][batch_idx] = micro_loss.detach()\n",
    "\n",
    "    if open_set:\n",
    "        \n",
    "        # Our decoding:\n",
    "        inputs_for_os = micro_logits[unknown_1_mask]\n",
    "        predicted_unknown_1s = micro_anomaly_detector(\n",
    "            inputs_for_os[:,known_classes_mask]).squeeze(-1)\n",
    "        \n",
    "        # open-set loss:\n",
    "        dec_1_loss_b = decoder_1b_criterion(\n",
    "            predicted_unknown_1s,\n",
    "            zda_mask[unknown_1_mask].float())\n",
    "\n",
    "        # Open set confusion matrix\n",
    "        os_cm_1 += utils.efficient_os_cm(\n",
    "            preds=(predicted_unknown_1s.detach() > 0.5).long(),\n",
    "            targets=zda_mask[unknown_1_mask].long()\n",
    "            )\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Bovenzi OS decoding:\n",
    "        predicted_unknown_1s = micro_anomaly_detector(\n",
    "            micro_logits[unknown_1_mask].max(1)[0])\n",
    "        \n",
    "        # open-set loss:\n",
    "        dec_1_loss_b = decoder_1b_criterion(\n",
    "            1 - predicted_unknown_1s,\n",
    "            zda_mask[unknown_1_mask].float())\n",
    "\n",
    "        # Open set confusion matrix\n",
    "        os_cm_1 += utils.efficient_os_cm(\n",
    "            preds=(predicted_unknown_1s.detach() == 0).long(),\n",
    "            targets=zda_mask[unknown_1_mask].long()\n",
    "            )\n",
    "        \"\"\"\n",
    "        \n",
    "        OS_b_acc = utils.get_balanced_accuracy(\n",
    "                    os_cm=os_cm_1,\n",
    "                    n_w=balanced_acc_n_w\n",
    "                    )\n",
    "\n",
    "        metrics_dict['losses_1b'][batch_idx] = dec_1_loss_b.detach()\n",
    "        metrics_dict['OS_B_accuracies'][batch_idx] = OS_b_acc\n",
    "\n",
    "    return micro_loss, \\\n",
    "        dec_1_loss_b, \\\n",
    "        micro_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64315009-be7c-40b7-8b80-a65471be9a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def macro_classif(\n",
    "        sample_batch,\n",
    "        batch_idx,\n",
    "        closed_set=True,\n",
    "        open_set=True):\n",
    "\n",
    "    global cs_cm_2\n",
    "    global os_cm_2\n",
    "    global metrics_dict\n",
    "\n",
    "    # get masks: THESE ARE NOT COMPLEMETARY!\n",
    "    type_A_mask, known_macro_classes_mask, \\\n",
    "        unknown_2_mask, active_query_mask_2 = utils.get_masks_2(\n",
    "            sample_batch[1],\n",
    "            N_QUERY,\n",
    "            device=device)\n",
    "\n",
    "    dec_2_loss_b = None\n",
    "    macro_loss = None\n",
    "    \n",
    "    # forward passes\n",
    "    macro_logits = macro_classifier(sample_batch[0])\n",
    "    \n",
    "    # get one_hot_labels:\n",
    "    oh_labels = utils.get_oh_labels(\n",
    "        decimal_labels=sample_batch[1][:, 0].long(),\n",
    "        total_classes=max_prototype_buffer_macro,\n",
    "        device=device)\n",
    "    \n",
    "    # accuracies:\n",
    "    CS_acc_2 = utils.get_acc(\n",
    "        logits_preds=macro_logits[active_query_mask_2].detach(),\n",
    "        oh_labels=oh_labels[active_query_mask_2])\n",
    "\n",
    "    # Closed set confusion matrix\n",
    "    cs_cm_2 = cs_cm_2 + utils.efficient_cm(\n",
    "        preds=macro_logits[active_query_mask_2].detach(),\n",
    "        targets=sample_batch[1][:, 0][active_query_mask_2].long())\n",
    "    \n",
    "    metrics_dict['CS_2_accuracies'][batch_idx] = CS_acc_2.detach()\n",
    "\n",
    "    if closed_set:\n",
    "        \n",
    "        # loss computation\n",
    "        macro_loss = macro_multiclass_error(\n",
    "            macro_logits[active_query_mask_2],\n",
    "            sample_batch[1][:, 0][active_query_mask_2].long())\n",
    "\n",
    "        # for reporting:\n",
    "        metrics_dict['losses_2a'][batch_idx] = macro_loss.detach()\n",
    "\n",
    "    if open_set:\n",
    "        \n",
    "        # our decoding:\n",
    "        inputs_for_decoding = macro_logits[unknown_2_mask]\n",
    "        predicted_unknown_2s = macro_anomaly_detector(\n",
    "            inputs_for_decoding[:,known_macro_classes_mask]).squeeze(-1)\n",
    "            \n",
    "        # open-set loss:\n",
    "        dec_2_loss_b = decoder_2b_criterion(\n",
    "            predicted_unknown_2s,\n",
    "            type_A_mask[unknown_2_mask].float())\n",
    "\n",
    "        # Open set confusion matrix\n",
    "        os_cm_2 += utils.efficient_os_cm(\n",
    "            preds=(predicted_unknown_2s.detach() > 0.5).long(),\n",
    "            targets=type_A_mask[unknown_2_mask].long()\n",
    "            )\n",
    "        \"\"\"\n",
    "        \n",
    "        # Bovenzi decoding:\n",
    "        predicted_unknown_2s = macro_anomaly_detector(\n",
    "            macro_logits[unknown_2_mask].max(1)[0])\n",
    "        \n",
    "        # open-set loss:\n",
    "        dec_2_loss_b = decoder_2b_criterion(\n",
    "            1 - predicted_unknown_2s,\n",
    "            type_A_mask[unknown_2_mask].float())\n",
    "\n",
    "        # Open set confusion matrix\n",
    "        os_cm_2 += utils.efficient_os_cm(\n",
    "            preds=(predicted_unknown_2s.detach() == 0).long(),\n",
    "            targets=type_A_mask[unknown_2_mask].long()\n",
    "            )\n",
    "        \"\"\"\n",
    "        \n",
    "        OS_2_B_acc = utils.get_balanced_accuracy(\n",
    "                    os_cm=os_cm_2,\n",
    "                    n_w=balanced_acc_n_w\n",
    "                    )\n",
    "\n",
    "        metrics_dict['losses_2b'][batch_idx] = dec_2_loss_b.detach()\n",
    "        metrics_dict['OS_2_B_accuracies'][batch_idx] = OS_2_B_acc.detach()\n",
    "\n",
    "    return macro_loss, \\\n",
    "        dec_2_loss_b, \\\n",
    "        macro_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d118e201-21e9-454c-9837-93f705a9195c",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## init data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c48d63b9-982b-4e04-ad2b-bb0d8271a971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "natural_inputs_dim = 46\n",
    "save = False\n",
    "wb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab974a58-3d4f-4ed8-90ef-433225757430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "torch_seed = 777\n",
    "torch.manual_seed(torch_seed)\n",
    "\n",
    "#\n",
    "#\n",
    "# Initialize\n",
    "#\n",
    "#\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dataset and Dataloader:\n",
    "train_dataset = dm.RealFewShotDataset_LowDim(\n",
    "    features=train_data.drop(columns=[\n",
    "        'Micro Label',\n",
    "        'Macro Label',\n",
    "        'ZdA',\n",
    "        'Type_A_ZdA',\n",
    "        'Type_B_ZdA']).values,\n",
    "    df=train_data,\n",
    "    micro_label_enc=micro_label_encoder,\n",
    "    macro_label_enc=macro_label_encoder)\n",
    "\n",
    "test_dataset = dm.RealFewShotDataset_LowDim(\n",
    "    features=test_data.drop(columns=[\n",
    "        'Micro Label',\n",
    "        'Macro Label',\n",
    "        'ZdA',\n",
    "        'Type_A_ZdA',\n",
    "        'Type_B_ZdA']).values,\n",
    "    df=test_data,\n",
    "    micro_label_enc=micro_label_encoder,\n",
    "    macro_label_enc=macro_label_encoder)\n",
    "\n",
    "\n",
    "# Number of classes per task :\n",
    "# two of them are ZdAs, one is a type B and the other a type A\n",
    "N_WAY = 4\n",
    "N_SHOT = 5   # Number of samples per class in the support set\n",
    "N_QUERY = 15  # Number of samples per class in the query set\n",
    "\n",
    "n_train_tasks = 500    # For speedy tests, reduce here...\n",
    "n_eval_tasks = 50     # For speedy tests, reduce here...\n",
    "\n",
    "\n",
    "num_of_test_micro_classes = len(test_dataset.micro_classes)\n",
    "train_micro_classes = train_dataset.micro_classes\n",
    "num_of_train_micro_classes = len(train_micro_classes)\n",
    "train_macro_classes = np.unique(train_dataset.macro_labels)\n",
    "num_of_train_macro_classes = len(train_macro_classes)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    sampler=dm.FewShotSampler(\n",
    "                dataset=train_dataset,\n",
    "                n_tasks=n_train_tasks,\n",
    "                classes_per_it=N_WAY,\n",
    "                k_shot=N_SHOT,\n",
    "                q_shot=N_QUERY),\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    collate_fn=dm.convenient_cf)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    sampler=dm.FewShotSampler(\n",
    "                dataset=test_dataset,\n",
    "                n_tasks=n_eval_tasks,\n",
    "                classes_per_it=N_WAY,\n",
    "                k_shot=N_SHOT,\n",
    "                q_shot=N_QUERY),\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    collate_fn=dm.convenient_cf)\n",
    "\n",
    "\n",
    "# reproducibility\n",
    "train_loader.sampler.reset()\n",
    "test_loader.sampler.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1db0a7-93b7-403d-92ec-b9bb22959cad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## init architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0fd612f-906b-4168-9bb8-bddad2a06cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Hyper params:\n",
    "###\n",
    "max_prototype_buffer_micro = 40\n",
    "max_prototype_buffer_macro = 10\n",
    "lr = 0.001\n",
    "# training parameters\n",
    "n_epochs = 100\n",
    "norm = \"batch\"\n",
    "dropout = 0.1\n",
    "patience = 60\n",
    "lambda_os = \"NAN\"\n",
    "\n",
    "processor_attention_heads = \"NAN\"\n",
    "h_dim = 1024\n",
    "report_step_frequency = 100\n",
    "\n",
    "pos_weight_1 = 1\n",
    "pos_weight_2 = 3\n",
    "\n",
    "architectures = 'MLP + relu (learnable)'\n",
    "balanced_acc_n_w = 0.5\n",
    "attr_w = \"NAN\"\n",
    "rep_w = \"NAN\"\n",
    "tau_u = 0.85\n",
    "run_name = f'Bvnzi confec online'\n",
    "\n",
    "if wb:\n",
    "    wandb.init(project='Nero_1.1',\n",
    "               name=run_name,\n",
    "               config={\"N_SHOT\": N_SHOT,\n",
    "                       \"N_QUERY\": N_QUERY,\n",
    "                       \"N_WAY\": N_WAY,\n",
    "                       \"num_of_test_classes\": num_of_test_micro_classes,\n",
    "                       \"num_of_train_classes\": num_of_train_micro_classes,\n",
    "                       \"train_batch_size\": N_WAY * (N_SHOT + N_QUERY),\n",
    "                       \"len(train_loader)\": train_loader.sampler.n_tasks,\n",
    "                       \"len(test_dataset)\": test_loader.sampler.n_tasks,\n",
    "                       \"max_prototype_buffer_micro\": max_prototype_buffer_micro,\n",
    "                       \"max_prototype_buffer_macro\": max_prototype_buffer_macro,\n",
    "                       \"device\": device,\n",
    "                       \"natural_inputs_dim\": natural_inputs_dim,\n",
    "                       \"h_dim\": h_dim,\n",
    "                       \"lr\": lr,\n",
    "                       \"n_epochs\": n_epochs,\n",
    "                       \"norm\": norm,\n",
    "                       \"dropout\": dropout,\n",
    "                       \"patience\": patience,\n",
    "                       'zdas': data[data.ZdA == True]['Micro Label'].unique(),\n",
    "                       \"lambda_os\": lambda_os,\n",
    "                       \"positive_weight_1\": pos_weight_1,\n",
    "                       \"positive_weight_2\": pos_weight_2,\n",
    "                       \"architectures\": architectures,\n",
    "                       \"balanced_acc_n_w\": balanced_acc_n_w,\n",
    "                       \"attr_w\": attr_w,\n",
    "                       \"rep_w\": rep_w\n",
    "                       })\n",
    "else:\n",
    "    print(run_name)\n",
    "\n",
    "\n",
    "# Initialize the autoencoder model, loss function, and optimizer\n",
    "micro_classifier = models.Simple_MLP_Classifier(\n",
    "    input_size=natural_inputs_dim,\n",
    "    hidden_size=h_dim,\n",
    "    output_size=max_prototype_buffer_micro,\n",
    "    dropout=dropout).to(torch.float64).to(device)\n",
    "\n",
    "\"\"\"\n",
    "micro_anomaly_detector = models.LearnableAnomalyDetectionModule(tau_u)\\\n",
    "    .to(torch.float64).to(device)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "micro_anomaly_detector = models.FixedAnomalyDetectionModule(tau_u)\\\n",
    "    .to(torch.float64).to(device)\n",
    "\n",
    "\"\"\"\n",
    "micro_anomaly_detector = models.Confidence_Decoder(\n",
    "                in_dim=N_WAY-2,  # Subtract 1 ZdA\n",
    "                dropout=dropout,\n",
    "                device=device\n",
    "                ).to(torch.float64).to(device)\n",
    "\n",
    "decoder_1b_criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.Tensor([pos_weight_1])).to(device)\n",
    "\n",
    "micro_multiclass_error = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "macro_classifier = models.Simple_MLP_Classifier(\n",
    "    input_size=natural_inputs_dim,\n",
    "    hidden_size=h_dim,\n",
    "    output_size=max_prototype_buffer_macro,\n",
    "    dropout=dropout).to(torch.float64).to(device)\n",
    "\n",
    "\"\"\"\n",
    "macro_anomaly_detector = models.LearnableAnomalyDetectionModule(tau_u)\\\n",
    "    .to(torch.float64).to(device)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "macro_anomaly_detector = models.FixedAnomalyDetectionModule(tau_u)\\\n",
    "    .to(torch.float64).to(device)\n",
    "\"\"\"\n",
    "macro_anomaly_detector = models.Confidence_Decoder(\n",
    "                in_dim=N_WAY-1,\n",
    "                dropout=dropout,\n",
    "                device=device\n",
    "                ).to(torch.float64).to(device)\n",
    "\n",
    "decoder_2b_criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.Tensor([pos_weight_2])).to(device)\n",
    "\n",
    "macro_multiclass_error = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n",
    "params_for_optimizer = \\\n",
    "        list(macro_classifier.parameters()) + \\\n",
    "        list(micro_classifier.parameters()) + \\\n",
    "        list(micro_anomaly_detector.parameters()) + \\\n",
    "        list(macro_anomaly_detector.parameters())\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    params_for_optimizer,\n",
    "    lr=lr)\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "max_acc_classif_micro = torch.zeros(1, device=device)\n",
    "epochs_without_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52732bd3-fde9-43ef-a087-fd4ddb11527d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(macro_classifier)\n",
    "wandb.watch(micro_classifier)\n",
    "wandb.watch(macro_anomaly_detector)\n",
    "wandb.watch(micro_anomaly_detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3eeb4-edb8-499a-a8ad-c24bf70f417e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train Online:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d1b2f06-6875-414a-8ffd-2f254d0d8f79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc217f5f2be4c92bd32144833cf5fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_acc_classif_micro: 0.9520000219345093\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a159ff84296402cbca9a5503eadda3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='32.129 MB of 32.129 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>CS1 accuracy_eval: </td><td>▃█▇▇█▇▁██▂▆▆██▆▇█▃▃▄▄▁▇█▃▃█▃▇████▇▂█▆█▆▂</td></tr><tr><td>CS1 accuracy_train: </td><td>▆█▆▆▆▆▆▆▆▇▅▆▁▆▆▆▆▅▆▆▅▇▆█▆▆▅▆▅▆▆█▆▆▅▆▆▆▆█</td></tr><tr><td>CS2 accuracy_eval: </td><td>▁█▃▃▃▄▅█▃▁▃▂▇▇█▃▃▂▆▂▂▅▃▄▆██▃▂▃▃▃▄▃▃▃▃█▂▃</td></tr><tr><td>CS2 accuracy_train: </td><td>▄█▅▆▆▅▅▆▆▇▅▅▁▅▅▆▅▅▆▅▅▆▅█▅▆▇▇▇▆▆█▇▆▅▇▆▆▆▇</td></tr><tr><td>OS1 Bal. accuracy_eval: </td><td>▂▅▅▅▅▅▆█▅▂▃▅███▅█▇▇▄▂▃▅█▄▄█▁▅▅▅██▅▁█▅█▅▁</td></tr><tr><td>OS1 Bal. accuracy_train: </td><td>▁▄▆▆▆▆▆▇▆▆▆▆▆▆▆▆▆▆▆▆▅▆▆█▆▆▆▆▆▆▆▅▆▆▅▆▆▆▆▆</td></tr><tr><td>OS1 accuracy_eval: </td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>OS1 accuracy_train: </td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>OS2 Bal. accuracy_eval: </td><td>▂▄▁██▇▇▇█▇▇█▆▆▇▅▇▅▇▅▇▆█▅▆▆▆▇███▆▅█▇▆█▆█▇</td></tr><tr><td>OS2 Bal. accuracy_train: </td><td>▂▁▂▂▄▄▄▄▃▃▄▄▆▄▄▄▄▄▄▄▅▅▅█▅▅▅▅▅▅▅█▆▅▇▅▅▆▆█</td></tr><tr><td>OS2 accuracy_eval: </td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>OS2 accuracy_train: </td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch: </td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>mean dec_1_loss_a_eval: </td><td>▇▁▂▂▁▂█▁▁▇▃▃▂▁▃▂▁▆▆▅▅█▂▁▆▆▁▆▂▁▁▁▁▂▇▁▃▁▃▇</td></tr><tr><td>mean dec_1_loss_a_train: </td><td>▅▁▃▃▃▃▃▃▃▂▄▃█▃▃▃▃▄▃▃▅▂▃▁▃▃▄▃▄▃▃▁▃▃▄▃▃▃▃▁</td></tr><tr><td>mean dec_1_loss_b_eval: </td><td>▆▅▅▅▄▄▃▃▄▇▆▄▁▂▁▄▁▂▃▅▇▆▄▁▅▅▁█▄▄▄▁▁▄█▁▄▁▄█</td></tr><tr><td>mean dec_1_loss_b_train: </td><td>█▇▅▅▄▄▄▃▃▃▃▃▄▃▃▃▃▃▃▃▅▃▃▁▃▃▃▃▃▃▃▄▃▃▅▃▃▃▃▃</td></tr><tr><td>mean dec_2_loss_a_eval: </td><td>█▁▆▆▆▅▄▁▆█▆▇▂▂▁▆▆▇▃▇▇▄▆▅▃▁▁▆▇▆▆▆▅▆▆▆▆▁▇▆</td></tr><tr><td>mean dec_2_loss_a_train: </td><td>▆▁▄▃▃▄▄▃▃▂▄▄█▄▄▃▄▄▃▄▅▃▄▁▄▃▂▂▂▃▃▁▂▃▄▂▃▃▃▂</td></tr><tr><td>mean dec_2_loss_b_eval: </td><td>█▇█▅▅▆▅▅▄▆▆▄▅▅▅▅▆█▅█▆▆▃▇▅▅▄▄▂▁▁▄▇▁▄▅▁▄▁▄</td></tr><tr><td>mean dec_2_loss_b_train: </td><td>█▇▇▆▇▆▆▆▆▅▆▆▄▅▆▅▆▅▅▄▅▄▅▁▄▄▄▅▃▅▄▁▄▅▂▅▄▄▄▁</td></tr><tr><td>mean proc_reg_loss1 eval: </td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean proc_reg_loss1 train: </td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean proc_reg_loss2 eval: </td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean proc_reg_loss2 train: </td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step: </td><td>█▆▃▁█▃▁█▆▁█▆▅▁▆▅▁▁▆▃▁▆▅▁█▅▃▁▆▃▁█▃▁█▆▃█▆▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>CS1 accuracy_eval: </td><td>0.73333</td></tr><tr><td>CS1 accuracy_train: </td><td>0.91782</td></tr><tr><td>CS2 accuracy_eval: </td><td>0.66667</td></tr><tr><td>CS2 accuracy_train: </td><td>0.95776</td></tr><tr><td>OS1 Bal. accuracy_eval: </td><td>0.36667</td></tr><tr><td>OS1 Bal. accuracy_train: </td><td>0.87676</td></tr><tr><td>OS1 accuracy_eval: </td><td>0.0</td></tr><tr><td>OS1 accuracy_train: </td><td>0.0</td></tr><tr><td>OS2 Bal. accuracy_eval: </td><td>0.83333</td></tr><tr><td>OS2 Bal. accuracy_train: </td><td>0.69266</td></tr><tr><td>OS2 accuracy_eval: </td><td>0.0</td></tr><tr><td>OS2 accuracy_train: </td><td>0.0</td></tr><tr><td>epoch: </td><td>99</td></tr><tr><td>mean dec_1_loss_a_eval: </td><td>2.99761</td></tr><tr><td>mean dec_1_loss_a_train: </td><td>2.81307</td></tr><tr><td>mean dec_1_loss_b_eval: </td><td>0.764</td></tr><tr><td>mean dec_1_loss_b_train: </td><td>0.5342</td></tr><tr><td>mean dec_2_loss_a_eval: </td><td>1.79448</td></tr><tr><td>mean dec_2_loss_a_train: </td><td>1.50339</td></tr><tr><td>mean dec_2_loss_b_eval: </td><td>0.93059</td></tr><tr><td>mean dec_2_loss_b_train: </td><td>1.00328</td></tr><tr><td>mean proc_reg_loss1 eval: </td><td>0.0</td></tr><tr><td>mean proc_reg_loss1 train: </td><td>0.0</td></tr><tr><td>mean proc_reg_loss2 eval: </td><td>0.0</td></tr><tr><td>mean proc_reg_loss2 train: </td><td>0.0</td></tr><tr><td>step: </td><td>400</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Bvnzi confec online</strong> at: <a href='https://wandb.ai/jfcevallos/Nero_1.1/runs/q6ghwzib' target=\"_blank\">https://wandb.ai/jfcevallos/Nero_1.1/runs/q6ghwzib</a><br/>Synced 7 W&B file(s), 800 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231230_141940-q6ghwzib/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    # TRAIN\n",
    "    macro_classifier.train()\n",
    "    micro_classifier.train()\n",
    "    macro_anomaly_detector.train()\n",
    "    micro_anomaly_detector.train()\n",
    "\n",
    "    # reset conf Mats\n",
    "    cs_cm_1 = torch.zeros(\n",
    "        [max_prototype_buffer_micro, max_prototype_buffer_micro],\n",
    "        device=device)\n",
    "    os_cm_1 = torch.zeros([2, 2], device=device)\n",
    "    cs_cm_2 = torch.zeros(\n",
    "        [max_prototype_buffer_macro, max_prototype_buffer_macro],\n",
    "        device=device)\n",
    "    os_cm_2 = torch.zeros([2, 2], device=device)\n",
    "\n",
    "    # reset metrics dict\n",
    "    metrics_dict = utils.reset_metrics_dict_optimized(\n",
    "        train_loader.sampler.n_tasks,\n",
    "        device)\n",
    "\n",
    "    for batch_idx, sample_batch in enumerate(train_loader):\n",
    "        # go to cuda:\n",
    "        sample_batch = sample_batch[0].to(device), sample_batch[1].to(device)\n",
    "\n",
    "        micro_loss, os_1_loss, micro_logits = micro_classif(\n",
    "            sample_batch,\n",
    "            batch_idx)\n",
    "\n",
    "        macro_loss, os_2_loss, macro_logits = macro_classif(\n",
    "            sample_batch,\n",
    "            batch_idx)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss = micro_loss + macro_loss + os_1_loss + os_2_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Reporting\n",
    "        step = batch_idx + (epoch * train_loader.sampler.n_tasks)\n",
    "\n",
    "        if step % report_step_frequency == 0:\n",
    "            utils.reporting_simple_optimized(\n",
    "                'train',\n",
    "                epoch,\n",
    "                metrics_dict,\n",
    "                batch_idx,\n",
    "                report_step_frequency,\n",
    "                wb,\n",
    "                wandb)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        # Evaluation\n",
    "        macro_classifier.eval()\n",
    "        micro_classifier.eval()\n",
    "        macro_anomaly_detector.eval()\n",
    "        micro_anomaly_detector.eval()\n",
    "\n",
    "        # reset conf Mats\n",
    "        cs_cm_1 = torch.zeros(\n",
    "            [max_prototype_buffer_micro, max_prototype_buffer_micro],\n",
    "            device=device)\n",
    "        os_cm_1 = torch.zeros([2, 2], device=device)\n",
    "        cs_cm_2 = torch.zeros(\n",
    "            [max_prototype_buffer_macro, max_prototype_buffer_macro],\n",
    "            device=device)\n",
    "        os_cm_2 = torch.zeros([2, 2], device=device)\n",
    "\n",
    "        # reset metrics dict\n",
    "        metrics_dict = utils.reset_metrics_dict_optimized(\n",
    "            test_loader.sampler.n_tasks,\n",
    "            device)\n",
    "\n",
    "        # go!\n",
    "        for batch_idx, sample_batch in enumerate(test_loader):\n",
    "            # go to cuda:\n",
    "            sample_batch = sample_batch[0].to(device), sample_batch[1].to(device)\n",
    "\n",
    "            micro_loss, os_1_loss, micro_logits = micro_classif(\n",
    "                sample_batch,\n",
    "                batch_idx)\n",
    "\n",
    "            macro_loss, os_2_loss, macro_logits = macro_classif(\n",
    "                sample_batch,\n",
    "                batch_idx)\n",
    "\n",
    "            # Reporting\n",
    "            step = batch_idx + (epoch * test_loader.sampler.n_tasks)\n",
    "\n",
    "            if step % report_step_frequency == 0:\n",
    "                utils.reporting_simple_optimized(\n",
    "                    'eval',\n",
    "                    epoch,\n",
    "                    metrics_dict,\n",
    "                    batch_idx,\n",
    "                    report_step_frequency,\n",
    "                    wb,\n",
    "                    wandb)\n",
    "\n",
    "        pu.super_plotting_function(\n",
    "                phase='Evaluation',\n",
    "                labels=sample_batch[1].cpu(),\n",
    "                hiddens_1=micro_logits.detach().cpu(),\n",
    "                hiddens_2=macro_logits.detach().cpu(),\n",
    "                scores_1=micro_logits.detach().cpu(),\n",
    "                scores_2=macro_logits.detach().cpu(),\n",
    "                cs_cm_1=cs_cm_1.cpu(),\n",
    "                cs_cm_2=cs_cm_2.cpu(),\n",
    "                os_cm_1=os_cm_1.cpu(),\n",
    "                os_cm_2=os_cm_2.cpu(),\n",
    "                wb=wb,\n",
    "                wandb=wandb,\n",
    "                complete_micro_classes=micro_classes,\n",
    "                complete_macro_classes=macro_classes\n",
    "            )\n",
    "        \n",
    "        # Checking for improvement\n",
    "        curr_acc_classif_micro = metrics_dict['CS_accuracies'].mean().item()\n",
    "\n",
    "        if curr_acc_classif_micro > max_acc_classif_micro:\n",
    "            max_acc_classif_micro = curr_acc_classif_micro\n",
    "            epochs_without_improvement = 0\n",
    "            if save:\n",
    "                print(f'saving models at epoch {epoch}')\n",
    "                save_stuff(run_name)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        '''\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping at episode {step}')\n",
    "            if wb:\n",
    "                wandb.log({'Early stopping at episode': step})\n",
    "            break\n",
    "        '''\n",
    "print(f'max_acc_classif_micro: {max_acc_classif_micro}')\n",
    "\n",
    "if wb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9831d3-5748-4ac4-9240-2079d5b83c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61185f24-7cd5-4496-bdf2-e3fce00999e3",
   "metadata": {},
   "source": [
    "# Train (Offline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de4099b-44d4-46df-939c-1b17b7ecfaf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## first phase: closed set classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e67cac-f075-47ae-8d11-0ecc012d84c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    # TRAIN\n",
    "    macro_classifier.train()\n",
    "    micro_classifier.train()\n",
    "\n",
    "    # reset conf Mats\n",
    "    cs_cm_1 = torch.zeros(\n",
    "        [max_prototype_buffer_micro, max_prototype_buffer_micro],\n",
    "        device=device)\n",
    "    cs_cm_2 = torch.zeros(\n",
    "        [max_prototype_buffer_macro, max_prototype_buffer_macro],\n",
    "        device=device)\n",
    "\n",
    "    # reset metrics dict\n",
    "    metrics_dict = utils.reset_metrics_dict_optimized(\n",
    "        train_loader.sampler.n_tasks,\n",
    "        device)\n",
    "\n",
    "    for batch_idx, sample_batch in enumerate(train_loader):\n",
    "        # go to cuda:\n",
    "        sample_batch = sample_batch[0].to(device), sample_batch[1].to(device)\n",
    "\n",
    "        micro_loss, _, micro_logits = micro_classif(\n",
    "            sample_batch,\n",
    "            batch_idx,\n",
    "            True,\n",
    "            False)\n",
    "\n",
    "        macro_loss, _, macro_logits = macro_classif(\n",
    "            sample_batch,\n",
    "            batch_idx,\n",
    "            True,\n",
    "            False)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss = micro_loss + macro_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Reporting\n",
    "        step = batch_idx + (epoch * train_loader.sampler.n_tasks)\n",
    "\n",
    "        if step % report_step_frequency == 0:\n",
    "            utils.reporting_simple_optimized(\n",
    "                'train',\n",
    "                epoch,\n",
    "                metrics_dict,\n",
    "                batch_idx,\n",
    "                report_step_frequency,\n",
    "                wb,\n",
    "                wandb)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        # Evaluation\n",
    "        macro_classifier.eval()\n",
    "        micro_classifier.eval()\n",
    "\n",
    "        # reset conf Mats\n",
    "        cs_cm_1 = torch.zeros(\n",
    "            [max_prototype_buffer_micro, max_prototype_buffer_micro],\n",
    "            device=device)\n",
    "        cs_cm_2 = torch.zeros(\n",
    "            [max_prototype_buffer_macro, max_prototype_buffer_macro],\n",
    "            device=device)\n",
    "\n",
    "        # reset metrics dict\n",
    "        metrics_dict = utils.reset_metrics_dict_optimized(\n",
    "            test_loader.sampler.n_tasks,\n",
    "            device)\n",
    "\n",
    "        # go!\n",
    "        for batch_idx, sample_batch in enumerate(test_loader):\n",
    "            # go to cuda:\n",
    "            sample_batch = sample_batch[0].to(device), sample_batch[1].to(device)\n",
    "\n",
    "            micro_loss, _, micro_logits = micro_classif(\n",
    "                sample_batch,\n",
    "                batch_idx,\n",
    "                True,\n",
    "                False)\n",
    "\n",
    "            macro_loss, _, macro_logits = macro_classif(\n",
    "                sample_batch,\n",
    "                batch_idx,\n",
    "                True,\n",
    "                False)\n",
    "\n",
    "            # Reporting\n",
    "            step = batch_idx + (epoch * test_loader.sampler.n_tasks)\n",
    "\n",
    "            if step % report_step_frequency == 0:\n",
    "                utils.reporting_simple_optimized(\n",
    "                    'eval',\n",
    "                    epoch,\n",
    "                    metrics_dict,\n",
    "                    batch_idx,\n",
    "                    report_step_frequency,\n",
    "                    wb,\n",
    "                    wandb)\n",
    "\n",
    "        # Checking for improvement\n",
    "        curr_acc_classif_micro = metrics_dict['CS_accuracies'].mean().item()\n",
    "\n",
    "        if curr_acc_classif_micro > max_acc_classif_micro:\n",
    "            max_acc_classif_micro = curr_acc_classif_micro\n",
    "            epochs_without_improvement = 0\n",
    "            if save:\n",
    "                print(f'saving models at epoch {epoch}')\n",
    "                save_stuff(run_name)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "\n",
    "print(f'max_acc_classif_micro: {max_acc_classif_micro}')\n",
    "\n",
    "if wb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239723a5-eaed-4f46-9aa8-7307a1ac691e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## second phase: open set classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4aa4c4-04fe-42b0-a639-4fd7f1a6dc52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "micro_classifier.load_state_dict(torch.load('Bovnz_micro_classifier.pt'))\n",
    "macro_classifier.load_state_dict(torch.load('Bovnz_macro_classifier.pt'))\n",
    "micro_classifier.eval()\n",
    "macro_classifier.eval()\n",
    "\n",
    "# Freeze the pre-trained processor\n",
    "for param in micro_classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "# Freeze the pre-trained processor\n",
    "for param in macro_classifier.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806b162c-0808-4d4e-93c1-f4e87d6b7937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    # TRAIN\n",
    "    macro_classifier.train()\n",
    "    micro_classifier.train()\n",
    "\n",
    "    # reset conf Mats\n",
    "    os_cm_1 = torch.zeros([2, 2], device=device)\n",
    "    os_cm_2 = torch.zeros([2, 2], device=device)\n",
    "\n",
    "    # reset metrics dict\n",
    "    metrics_dict = utils.reset_metrics_dict_optimized(\n",
    "        train_loader.sampler.n_tasks,\n",
    "        device)\n",
    "\n",
    "    for batch_idx, sample_batch in enumerate(train_loader):\n",
    "        # go to cuda:\n",
    "        sample_batch = sample_batch[0].to(device), sample_batch[1].to(device)\n",
    "\n",
    "        _, os_micro_loss, micro_logits = micro_classif(\n",
    "            sample_batch,\n",
    "            batch_idx,\n",
    "            False,\n",
    "            True)\n",
    "\n",
    "        _, os_macro_loss, macro_logits = macro_classif(\n",
    "            sample_batch,\n",
    "            batch_idx,\n",
    "            False,\n",
    "            True)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss = os_micro_loss + os_macro_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Reporting\n",
    "        step = batch_idx + (epoch * train_loader.sampler.n_tasks)\n",
    "\n",
    "        if step % report_step_frequency == 0:\n",
    "            utils.reporting_simple_optimized(\n",
    "                'train',\n",
    "                epoch,\n",
    "                metrics_dict,\n",
    "                batch_idx,\n",
    "                report_step_frequency,\n",
    "                wb,\n",
    "                wandb)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        # Evaluation\n",
    "        macro_classifier.eval()\n",
    "        micro_classifier.eval()\n",
    "\n",
    "        # reset conf Mats\n",
    "        os_cm_1 = torch.zeros([2, 2], device=device)\n",
    "        os_cm_2 = torch.zeros([2, 2], device=device)\n",
    "\n",
    "        # reset metrics dict\n",
    "        metrics_dict = utils.reset_metrics_dict_optimized(\n",
    "            test_loader.sampler.n_tasks,\n",
    "            device)\n",
    "\n",
    "        # go!\n",
    "        for batch_idx, sample_batch in enumerate(test_loader):\n",
    "            # go to cuda:\n",
    "            sample_batch = sample_batch[0].to(device), sample_batch[1].to(device)\n",
    "\n",
    "            micro_loss, _, micro_logits = micro_classif(\n",
    "                sample_batch,\n",
    "                batch_idx,\n",
    "                False,\n",
    "                True)\n",
    "\n",
    "            macro_loss, _, macro_logits = macro_classif(\n",
    "                sample_batch,\n",
    "                batch_idx,\n",
    "                False,\n",
    "                True)\n",
    "\n",
    "            # Reporting\n",
    "            step = batch_idx + (epoch * test_loader.sampler.n_tasks)\n",
    "\n",
    "            if step % report_step_frequency == 0:\n",
    "                utils.reporting_simple_optimized(\n",
    "                    'eval',\n",
    "                    epoch,\n",
    "                    metrics_dict,\n",
    "                    batch_idx,\n",
    "                    report_step_frequency,\n",
    "                    wb,\n",
    "                    wandb)\n",
    "\n",
    "        # Checking for improvement\n",
    "        curr_acc_classif_micro = metrics_dict['CS_accuracies'].mean().item()\n",
    "\n",
    "        if curr_acc_classif_micro > max_acc_classif_micro:\n",
    "            max_acc_classif_micro = curr_acc_classif_micro\n",
    "            epochs_without_improvement = 0\n",
    "            if save:\n",
    "                print(f'saving models at epoch {epoch}')\n",
    "                save_stuff(run_name)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "\n",
    "print(f'max_acc_classif_micro: {max_acc_classif_micro}')\n",
    "\n",
    "if wb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e9cdb8-0920-4150-bf4e-8a7fd2f86d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
