{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca5cd0f1-590d-49ce-9086-7e7ad55f98ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df4bc2-2dc3-49e1-94c7-b40b90325a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from connect import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3607f-0c58-48da-9267-5b8785d7298b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb3b5f-dc7f-4df5-a382-4c6120ee58b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import NAR.utils as utils\n",
    "import NAR.data_management as dm\n",
    "import NAR.plotting_utils as pu\n",
    "import NAR.masking as masking\n",
    "import NAR.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058da714-78cb-42e6-9d69-e38f36ade4ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Reload modules (DEV):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805529e4-73fb-4ac0-9124-d72b08aa7994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reload_modules([utils, dm, pu, masking, models])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711fdca5-ac7a-459a-a840-702f17a24f9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Checkpoint 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a91ea2-919c-4ed0-90ea-553941f5bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'data/pre_processed_unsw.csv',\n",
    "    low_memory=False)\n",
    "\n",
    "df = df[df['Micro Attack'] != -1]\n",
    "df['Micro Attack'] = df['Attack'] + '_' +df['Micro Attack'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bbfaa6-4fa7-4e6f-9f0b-6fbb299fe155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_columns_dict = {\n",
    "       'PROTOCOL': True, 'L7_PROTO': True, 'IN_BYTES': False,\n",
    "       'IN_PKTS': False, 'OUT_BYTES': False, 'OUT_PKTS': False, 'TCP_FLAGS': True, 'CLIENT_TCP_FLAGS': True,\n",
    "       'SERVER_TCP_FLAGS': True, 'FLOW_DURATION_MILLISECONDS': False, 'DURATION_IN': False,\n",
    "       'DURATION_OUT': False, 'MIN_TTL': False, 'MAX_TTL': False, 'LONGEST_FLOW_PKT': False,\n",
    "       'SHORTEST_FLOW_PKT': False, 'MIN_IP_PKT_LEN': False, 'MAX_IP_PKT_LEN': False,\n",
    "       'SRC_TO_DST_SECOND_BYTES': False, 'DST_TO_SRC_SECOND_BYTES': False,\n",
    "       'RETRANSMITTED_IN_BYTES': False, 'RETRANSMITTED_IN_PKTS': False,\n",
    "       'RETRANSMITTED_OUT_BYTES': False, 'RETRANSMITTED_OUT_PKTS': False,\n",
    "       'SRC_TO_DST_AVG_THROUGHPUT': False, 'DST_TO_SRC_AVG_THROUGHPUT': False,\n",
    "       'NUM_PKTS_UP_TO_128_BYTES': False, 'NUM_PKTS_128_TO_256_BYTES': False,\n",
    "       'NUM_PKTS_256_TO_512_BYTES': False, 'NUM_PKTS_512_TO_1024_BYTES': False,\n",
    "       'NUM_PKTS_1024_TO_1514_BYTES': False, 'TCP_WIN_MAX_IN': False, 'TCP_WIN_MAX_OUT': False,\n",
    "       'ICMP_TYPE': True, 'ICMP_IPV4_TYPE': True, 'DNS_QUERY_ID': False, 'DNS_QUERY_TYPE': True,\n",
    "       'DNS_TTL_ANSWER': True, 'FTP_COMMAND_RET_CODE': True}\n",
    "\n",
    "categorical_columns = pd.Series(categorical_columns_dict)\n",
    "categorical_columns = df.columns[list(np.where(np.array(categorical_columns)==True)[0])].tolist()\n",
    "cont_columns = list(set(df.columns.tolist()) - set(categorical_columns) - set(['Attack', 'Micro Attack']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ee6b0-320a-4825-ba67-7844987e0f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop(['Attack', 'Micro Attack'], axis=1)\n",
    "y = df[['Attack', 'Micro Attack']]\n",
    "\n",
    "categorical_indicator = X.dtypes == 'O'\n",
    "\n",
    "categorical_columns = X.columns[list(np.where(np.array(categorical_indicator)==True)[0])].tolist()\n",
    "cont_columns = list(set(X.columns.tolist()) - set(categorical_columns))\n",
    "\n",
    "cat_idxs = list(np.where(np.array(categorical_indicator)==True)[0])\n",
    "con_idxs = list(set(range(len(X.columns))) - set(cat_idxs))\n",
    "\n",
    "# Categories and target classes to natural numbers:\n",
    "cat_dims = []\n",
    "for col in categorical_columns:\n",
    "    l_enc = LabelEncoder()\n",
    "    X[col] = l_enc.fit_transform(X[col].values)\n",
    "    cat_dims.append(len(l_enc.classes_))\n",
    "\n",
    "X = X.values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "y = y.values\n",
    "macro_l_enc = LabelEncoder()\n",
    "micro_l_enc = LabelEncoder()\n",
    "macro_y = macro_l_enc.fit_transform(y[:, 0])\n",
    "micro_y = micro_l_enc.fit_transform(y[:, 1])\n",
    "\n",
    "# num of classes:\n",
    "num_macro_classes = len(np.unique(macro_y))\n",
    "num_micro_classes = len(np.unique(micro_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb49f2-ac00-45fa-8dc8-d9b4dff99f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting to suitable format:\n",
    "data = pd.DataFrame(X)\n",
    "data['Macro Label'] = macro_l_enc.inverse_transform(macro_y)\n",
    "data['Micro Label'] = micro_l_enc.inverse_transform(micro_y)\n",
    "\n",
    "micro_zdas = [\n",
    "        'Shellcode_0',                 # Type A\n",
    "        'Shellcode_1',                 # Type A\n",
    "        'Shellcode_2',                 # Type A\n",
    "        'Worms_0',                     # Type A\n",
    "        'Worms_1',                     # Type A\n",
    "        'Worms_2',                     # Type A\n",
    "        'Fuzzers_1',                   # Type B\n",
    "        'Reconnaissance_1',            # Type B\n",
    "        'Backdoor_0',                  # Type B\n",
    "        'Generic_0',                   # Type B\n",
    "        'DoS_0',                       # Type B\n",
    "        'Exploits_0',                  # Type B\n",
    "        'Analysis_0',                  # Type B\n",
    "        'Generic_1',                   # Type B\n",
    "        ]\n",
    "\n",
    "micro_type_A_ZdAs = [\n",
    "        'Shellcode_0',           # Type A\n",
    "        'Shellcode_1',           # Type A\n",
    "        'Shellcode_2',           # Type A\n",
    "        'Worms_0',               # Type A\n",
    "        'Worms_1',               # Type A\n",
    "        'Worms_2',               # Type A\n",
    "        ]\n",
    "\n",
    "micro_type_B_ZdAs = [\n",
    "        'Fuzzers_1',                   # Type B\n",
    "        'Reconnaissance_1',            # Type B\n",
    "        'Backdoor_0',                  # Type B\n",
    "        'Generic_0',                   # Type B\n",
    "        'DoS_0',                       # Type B\n",
    "        'Exploits_0',                  # Type B\n",
    "        'Analysis_0',                  # Type B\n",
    "        'Generic_1',                   # Type B\n",
    "        ]\n",
    "\n",
    "train_type_B_micro_classes = [\n",
    "        'Fuzzers_1',                   # Type B\n",
    "        'Reconnaissance_1',            # Type B\n",
    "        'Backdoor_0',                  # Type B\n",
    "        'Generic_0',                   # Type B\n",
    "        ]\n",
    "\n",
    "test_type_B_micro_classes = [\n",
    "        'DoS_0',                       # Type B\n",
    "        'Exploits_0',                  # Type B\n",
    "        'Analysis_0',                  # Type B\n",
    "        'Generic_1',                   # Type B\n",
    "        ]\n",
    "\n",
    "\n",
    "test_type_A_macro_classes = [\n",
    "        'Shellcode'            # Type A\n",
    "        ]\n",
    "\n",
    "train_type_A_macro_classes = [\n",
    "        'Worms'                # Type A\n",
    "        ]\n",
    "\n",
    "\n",
    "data = masking.mask_real_data_lowdim(\n",
    "    data=data,\n",
    "    micro_zdas=micro_zdas,\n",
    "    micro_type_A_ZdAs=micro_type_A_ZdAs,\n",
    "    micro_type_B_ZdAs=micro_type_B_ZdAs\n",
    "    )\n",
    "\n",
    "train_data, test_data = masking.split_real_data(\n",
    "    data,\n",
    "    train_type_B_micro_classes,\n",
    "    test_type_B_micro_classes,\n",
    "    test_type_A_macro_classes,\n",
    "    train_type_A_macro_classes\n",
    "    )\n",
    "\n",
    "micro_classes = data['Micro Label'].unique()\n",
    "macro_classes = data['Macro Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ca943-0d98-4e05-8d7d-6d2600a5a1e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train test split is psuedo-randomic. for this reason we do it just once.\n",
    "train_data.to_csv('data/unsw_train.csv', index=0)\n",
    "test_data.to_csv('data/unsw_test.csv', index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd34dc74-da12-4e51-93e5-f980352c995c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Checkpoint 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2307e-bb97-4603-b5e9-6991c27d0a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/unsw_train.csv', low_memory=False)\n",
    "test_data = pd.read_csv('data/unsw_test.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9a7b3-ab0a-4133-8c27-2fc2bf626c31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b41cf-4e0e-4ff8-a915-ab78ef737cc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## helper code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8e43c-c1a1-4228-8d98-60bb6d378981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_stuff(prefix):\n",
    "    torch.save(\n",
    "        encoder.state_dict(),\n",
    "        prefix+'enc.pt')\n",
    "    torch.save(\n",
    "        decoder_1_b.state_dict(),\n",
    "        prefix+'_dec_b.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc7dc5-62bb-4a98-80fc-a445b34a9c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def first_phase_simple(\n",
    "        sample_batch):\n",
    "\n",
    "    global cs_cm_1\n",
    "    global os_cm_1\n",
    "    global metrics_dict\n",
    "\n",
    "    # get masks: THESE ARE NOT COMPLEMETARY!\n",
    "    zda_mask, \\\n",
    "        known_classes_mask, \\\n",
    "        unknown_1_mask, \\\n",
    "        active_query_mask = utils.get_masks_1(\n",
    "            sample_batch[1],\n",
    "            N_QUERY,\n",
    "            device=device)\n",
    "\n",
    "    # get one_hot_labels:\n",
    "    oh_labels = utils.get_oh_labels(\n",
    "        decimal_labels=sample_batch[1][:, 1].long(),\n",
    "        total_classes=max_prototype_buffer_micro,\n",
    "        device=device)\n",
    "\n",
    "    # mask labels:\n",
    "    oh_masked_labels = utils.get_one_hot_masked_labels(\n",
    "        oh_labels,\n",
    "        unknown_1_mask,\n",
    "        device=device)\n",
    "\n",
    "    # encoding input space:\n",
    "    encoded_inputs = encoder(\n",
    "        sample_batch[0].float())\n",
    "\n",
    "    # processing\n",
    "    decoded_1, hiddens_1, predicted_kernel = processor_1(\n",
    "        encoded_inputs,\n",
    "        oh_masked_labels)\n",
    "\n",
    "    # semantic kernel:\n",
    "    semantic_kernel = oh_labels @ oh_labels.T\n",
    "    # Processor regularization:\n",
    "    proc_1_reg_loss = utils.get_kernel_kernel_loss(\n",
    "        semantic_kernel,\n",
    "        predicted_kernel,\n",
    "        a_w=attr_w,\n",
    "        r_w=rep_w)\n",
    "\n",
    "    # Transform lables for Few_shot Closed-set classif.\n",
    "    # compatible with the design of models.get_centroids functions,\n",
    "    # wich is called by our GAT processors.\n",
    "    unique_labels, transformed_labels = sample_batch[1][:, 1][active_query_mask].unique(\n",
    "        return_inverse=True)\n",
    "\n",
    "    # closed set classification\n",
    "    dec_1_loss_a = decoder_1a_criterion(\n",
    "        decoded_1[active_query_mask],\n",
    "        transformed_labels)\n",
    "\n",
    "    # Detach closed from open set gradients\n",
    "    input_for_os_dec = decoded_1.detach()\n",
    "    input_for_os_dec.requires_grad = True\n",
    "\n",
    "    # Unknown cluster prediction:\n",
    "    predicted_unknown_1s = decoder_1_b(\n",
    "        scores=input_for_os_dec[unknown_1_mask]\n",
    "        )\n",
    "\n",
    "    # open-set loss:\n",
    "    dec_1_loss_b = decoder_1b_criterion(\n",
    "        predicted_unknown_1s,\n",
    "        zda_mask[unknown_1_mask].float().unsqueeze(-1))\n",
    "\n",
    "    # inverse transform cs preds\n",
    "    it_preds = utils.inverse_transform_preds(\n",
    "        transormed_preds=decoded_1[active_query_mask],\n",
    "        real_labels=unique_labels,\n",
    "        real_class_num=max_prototype_buffer_micro)\n",
    "\n",
    "    #\n",
    "    # REPORTING:\n",
    "    #\n",
    "\n",
    "    # Closed set confusion matrix\n",
    "    cs_cm_1 += utils.efficient_cm(\n",
    "        preds=it_preds.detach(),\n",
    "        targets=sample_batch[1][:, 1][active_query_mask].long())\n",
    "\n",
    "    # Open set confusion matrix\n",
    "    os_cm_1 += utils.efficient_os_cm(\n",
    "        preds=(predicted_unknown_1s.detach() > 0.5).long(),\n",
    "        targets=zda_mask[unknown_1_mask].long()\n",
    "        )\n",
    "\n",
    "    # accuracies:\n",
    "    CS_acc = utils.get_acc(\n",
    "        logits_preds=it_preds,\n",
    "        oh_labels=oh_labels[active_query_mask])\n",
    "\n",
    "    OS_acc = utils.get_binary_acc(\n",
    "        logits=predicted_unknown_1s.detach(),\n",
    "        labels=zda_mask[unknown_1_mask].float().unsqueeze(-1))\n",
    "\n",
    "    OS_b_acc = utils.get_balanced_accuracy(\n",
    "                os_cm=os_cm_1,\n",
    "                n_w=balanced_acc_n_w\n",
    "                )\n",
    "\n",
    "    # for reporting:\n",
    "    metrics_dict['losses_1a'].append(dec_1_loss_a.item())\n",
    "    metrics_dict['proc_reg_loss1'].append(proc_1_reg_loss.item())\n",
    "    metrics_dict['CS_accuracies'].append(CS_acc.item())\n",
    "    metrics_dict['losses_1b'].append(dec_1_loss_b.item())\n",
    "    metrics_dict['OS_accuracies'].append(OS_acc.item())\n",
    "    metrics_dict['OS_B_accuracies'].append(OS_b_acc.item())\n",
    "\n",
    "    # Processor loss:\n",
    "    proc_1_loss = dec_1_loss_a + proc_1_reg_loss\n",
    "\n",
    "    return proc_1_loss, \\\n",
    "        dec_1_loss_b, \\\n",
    "        hiddens_1, \\\n",
    "        decoded_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd10944-4bac-47e1-a1a1-1fd0e9f056d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def second_phase_simple(\n",
    "        sample_batch,\n",
    "        hiddens_1):\n",
    "\n",
    "    global cs_cm_2\n",
    "    global os_cm_2\n",
    "    global metrics_dict\n",
    "\n",
    "    # get masks: THESE ARE NOT COMPLEMETARY!\n",
    "    type_A_mask, known_macro_classes_mask, \\\n",
    "        unknown_2_mask, active_query_mask_2 = utils.get_masks_2(\n",
    "            sample_batch[1],\n",
    "            N_QUERY,\n",
    "            device=device)\n",
    "\n",
    "    # get one_hot_labels:\n",
    "    oh_labels = utils.get_oh_labels(\n",
    "        decimal_labels=sample_batch[1][:, 0].long(),\n",
    "        total_classes=max_prototype_buffer_macro,\n",
    "        device=device)\n",
    "\n",
    "    # mask labels:\n",
    "    oh_masked_labels = utils.get_one_hot_masked_labels(\n",
    "        oh_labels,\n",
    "        unknown_2_mask,\n",
    "        device=device)\n",
    "\n",
    "    decoded_2, hiddens_2, predicted_kernel_2 = processor_2(\n",
    "        hiddens_1,\n",
    "        oh_masked_labels)\n",
    "\n",
    "    # semantic kernel:\n",
    "    semantic_kernel_2 = oh_labels @ oh_labels.T\n",
    "    # Processor regularization:\n",
    "    proc_2_reg_loss = utils.get_kernel_kernel_loss(\n",
    "        semantic_kernel_2,\n",
    "        predicted_kernel_2,\n",
    "        a_w=attr_w,\n",
    "        r_w=rep_w)\n",
    "\n",
    "    unique_macro_labels, transformed_labels_2 = sample_batch[1][:, 0][active_query_mask_2].unique(\n",
    "        return_inverse=True)\n",
    "\n",
    "    # Closed set: should learn to associate type B's to corr. macro cluster.\n",
    "    # geometrical \"break\" in the real-data case. (GRadients 2A)\n",
    "    dec_2_loss_a = decoder_2a_criterion(\n",
    "        decoded_2[active_query_mask_2],\n",
    "        transformed_labels_2)\n",
    "\n",
    "    input_for_os_dec_2 = decoded_2.detach()\n",
    "    input_for_os_dec_2.requires_grad = True\n",
    "\n",
    "    # Unknown cluster prediction:\n",
    "    predicted_unknown_2s = decoder_2_b(\n",
    "        scores=input_for_os_dec_2[unknown_2_mask]\n",
    "        )\n",
    "\n",
    "    # open-set loss:\n",
    "    dec_2_loss_b = decoder_2b_criterion(\n",
    "        predicted_unknown_2s,\n",
    "        type_A_mask[unknown_2_mask].float().unsqueeze(-1))\n",
    "\n",
    "    # inverse transform cs preds\n",
    "    it_preds = utils.inverse_transform_preds(\n",
    "        transormed_preds=decoded_2[active_query_mask_2],\n",
    "        real_labels=unique_macro_labels,\n",
    "        real_class_num=max_prototype_buffer_macro)\n",
    "\n",
    "    # Closed set confusion matrix\n",
    "    cs_cm_2 += utils.efficient_cm(\n",
    "        preds=it_preds.detach(),\n",
    "        targets=sample_batch[1][:, 0][active_query_mask_2].long(),\n",
    "        )\n",
    "\n",
    "    # Open set confusion matrix\n",
    "    os_cm_2 += utils.efficient_os_cm(\n",
    "        preds=(predicted_unknown_2s.detach() > 0.5).long(),\n",
    "        targets=type_A_mask[unknown_2_mask].long()\n",
    "        )\n",
    "\n",
    "    # accuracies:\n",
    "    CS_acc_2 = utils.get_acc(\n",
    "        logits_preds=it_preds,\n",
    "        oh_labels=oh_labels[active_query_mask_2])\n",
    "\n",
    "    OS_acc_2 = utils.get_binary_acc(\n",
    "        logits=predicted_unknown_2s.detach(),\n",
    "        labels=type_A_mask[unknown_2_mask].float().unsqueeze(-1))\n",
    "\n",
    "    OS_2_B_acc = utils.get_balanced_accuracy(\n",
    "                os_cm=os_cm_2,\n",
    "                n_w=balanced_acc_n_w\n",
    "                )\n",
    "\n",
    "    proc_2_loss = dec_2_loss_a + proc_2_reg_loss\n",
    "\n",
    "    # for reporting:\n",
    "    metrics_dict['losses_2a'].append(dec_2_loss_a.item())\n",
    "    metrics_dict['proc_reg_loss2'].append(proc_2_reg_loss.item())\n",
    "    metrics_dict['losses_2b'].append(dec_2_loss_b.item())\n",
    "    metrics_dict['CS_2_accuracies'].append(CS_acc_2.item())\n",
    "    metrics_dict['OS_2_accuracies'].append(OS_acc_2.item())\n",
    "    metrics_dict['OS_2_B_accuracies'].append(OS_2_B_acc.item())\n",
    "\n",
    "    return proc_2_loss, \\\n",
    "        dec_2_loss_b, \\\n",
    "        hiddens_2, \\\n",
    "        decoded_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d118e201-21e9-454c-9837-93f705a9195c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## init data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c48d63b9-982b-4e04-ad2b-bb0d8271a971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "natural_inputs_dim = 39\n",
    "save = True\n",
    "wb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab974a58-3d4f-4ed8-90ef-433225757430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "torch_seed = 1234\n",
    "torch.manual_seed(torch_seed)\n",
    "\n",
    "#\n",
    "#\n",
    "# Initialize\n",
    "#\n",
    "#\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dataset and Dataloader:\n",
    "train_dataset = dm.RealFewShotDataset_LowDim(\n",
    "    features=train_data.drop(columns=[\n",
    "        'Micro Label',\n",
    "        'Macro Label',\n",
    "        'ZdA',\n",
    "        'Type_A_ZdA',\n",
    "        'Type_B_ZdA']).values,\n",
    "    df=train_data)\n",
    "\n",
    "test_dataset = dm.RealFewShotDataset_LowDim(\n",
    "    features=test_data.drop(columns=[\n",
    "        'Micro Label',\n",
    "        'Macro Label',\n",
    "        'ZdA',\n",
    "        'Type_A_ZdA',\n",
    "        'Type_B_ZdA']).values,\n",
    "    df=test_data)\n",
    "\n",
    "# Number of classes per task :\n",
    "# two of them are ZdAs, one is a type B and the other a type A\n",
    "N_WAY = 5\n",
    "N_SHOT = 5   # Number of samples per class in the support set\n",
    "N_QUERY = 15  # Number of samples per class in the query set\n",
    "\n",
    "n_train_tasks = 500    # For speedy tests, reduce here...\n",
    "n_eval_tasks = 50     # For speedy tests, reduce here...\n",
    "\n",
    "\n",
    "num_of_test_classes = len(test_dataset.micro_classes)\n",
    "num_of_train_classes = len(train_dataset.micro_classes)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    sampler=dm.FewShotSampler(\n",
    "                dataset=train_dataset,\n",
    "                n_tasks=n_train_tasks,\n",
    "                classes_per_it=N_WAY,\n",
    "                k_shot=N_SHOT,\n",
    "                q_shot=N_QUERY),\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    collate_fn=dm.convenient_cf)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    sampler=dm.FewShotSampler(\n",
    "                dataset=test_dataset,\n",
    "                n_tasks=n_eval_tasks,\n",
    "                classes_per_it=N_WAY,\n",
    "                k_shot=N_SHOT,\n",
    "                q_shot=N_QUERY),\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    collate_fn=dm.convenient_cf)\n",
    "\n",
    "# reproducibility\n",
    "train_loader.sampler.reset()\n",
    "test_loader.sampler.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1db0a7-93b7-403d-92ec-b9bb22959cad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## init architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0fd612f-906b-4168-9bb8-bddad2a06cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/NERO/wandb/run-20231222_142033-pn5zf7ph</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jfcevallos/Nero_1.1/runs/pn5zf7ph' target=\"_blank\">UNSW from scratch</a></strong> to <a href='https://wandb.ai/jfcevallos/Nero_1.1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jfcevallos/Nero_1.1' target=\"_blank\">https://wandb.ai/jfcevallos/Nero_1.1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jfcevallos/Nero_1.1/runs/pn5zf7ph' target=\"_blank\">https://wandb.ai/jfcevallos/Nero_1.1/runs/pn5zf7ph</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###\n",
    "# Hyper params:\n",
    "###\n",
    "max_prototype_buffer_micro = 40\n",
    "max_prototype_buffer_macro = 10\n",
    "lr = 0.001\n",
    "# training parameters\n",
    "n_epochs = 60\n",
    "norm = \"batch\"\n",
    "dropout = 0.1\n",
    "patience = 20\n",
    "lambda_os = 1\n",
    "\n",
    "processor_attention_heads = 8\n",
    "h_dim = 1024\n",
    "report_step_frequency = 100\n",
    "\n",
    "pos_weight_1 = 2.5\n",
    "pos_weight_2 = 5\n",
    "\n",
    "architectures = 'GATV5 Confidence Dec'\n",
    "balanced_acc_n_w = 0.5\n",
    "attr_w = 1\n",
    "rep_w = 1\n",
    "run_name = f'UNSW from scratch'\n",
    "\n",
    "if wb:\n",
    "    wandb.init(project='Nero_1.1',\n",
    "               name=run_name,\n",
    "               config={\"N_SHOT\": N_SHOT,\n",
    "                       \"N_QUERY\": N_QUERY,\n",
    "                       \"N_WAY\": N_WAY,\n",
    "                       \"num_of_test_classes\": num_of_test_classes,\n",
    "                       \"num_of_train_classes\": num_of_train_classes,\n",
    "                       \"train_batch_size\": N_WAY * (N_SHOT + N_QUERY),\n",
    "                       \"len(train_loader)\": train_loader.sampler.n_tasks,\n",
    "                       \"len(test_dataset)\": test_loader.sampler.n_tasks,\n",
    "                       \"max_prototype_buffer_micro\": max_prototype_buffer_micro,\n",
    "                       \"max_prototype_buffer_macro\": max_prototype_buffer_macro,\n",
    "                       \"device\": device,\n",
    "                       \"natural_inputs_dim\": natural_inputs_dim,\n",
    "                       \"h_dim\": h_dim,\n",
    "                       \"lr\": lr,\n",
    "                       \"n_epochs\": n_epochs,\n",
    "                       \"norm\": norm,\n",
    "                       \"dropout\": dropout,\n",
    "                       \"patience\": patience,\n",
    "                       'zdas': data[data.ZdA == True]['Micro Label'].unique(),\n",
    "                       \"lambda_os\": lambda_os,\n",
    "                       \"positive_weight_1\": pos_weight_1,\n",
    "                       \"positive_weight_2\": pos_weight_2,\n",
    "                       \"architectures\": architectures,\n",
    "                       \"balanced_acc_n_w\": balanced_acc_n_w,\n",
    "                       \"attr_w\": attr_w,\n",
    "                       \"rep_w\": rep_w\n",
    "                       })\n",
    "else:\n",
    "    print(run_name)\n",
    "\n",
    "# Encoder\n",
    "encoder = models.Encoder(\n",
    "    in_features=natural_inputs_dim,\n",
    "    out_features=h_dim,\n",
    "    norm=norm,\n",
    "    dropout=dropout,\n",
    "    ).to(device)\n",
    "\n",
    "# First phase:\n",
    "processor_1 = models.GAT_V5_Processor(\n",
    "                h_dim=h_dim,\n",
    "                processor_attention_heads=processor_attention_heads,\n",
    "                dropout=dropout,\n",
    "                device=device\n",
    "                ).to(device)\n",
    "\n",
    "decoder_1a_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "decoder_1_b = models.Confidence_Decoder(\n",
    "                in_dim=N_WAY-2,  # Subtract 1 ZdA\n",
    "                dropout=dropout,\n",
    "                device=device\n",
    "                ).to(device)\n",
    "\n",
    "decoder_1b_criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.Tensor([pos_weight_1])).to(device)\n",
    "\n",
    "\n",
    "# Second phase:\n",
    "processor_2 = models.GAT_V5_Processor(\n",
    "                h_dim=h_dim,\n",
    "                processor_attention_heads=processor_attention_heads,\n",
    "                dropout=dropout,\n",
    "                device=device\n",
    "                ).to(device)\n",
    "\n",
    "decoder_2a_criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "decoder_2_b = models.Confidence_Decoder(\n",
    "                in_dim=N_WAY-1,\n",
    "                dropout=dropout,\n",
    "                device=device\n",
    "                ).to(device)\n",
    "\n",
    "decoder_2b_criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.Tensor([pos_weight_2])).to(device)\n",
    "\n",
    "\n",
    "params_for_processor_optimizer = \\\n",
    "        list(encoder.parameters()) + \\\n",
    "        list(processor_1.parameters()) + \\\n",
    "        list(processor_2.parameters())\n",
    "\n",
    "\n",
    "processor_optimizer = optim.Adam(\n",
    "    params_for_processor_optimizer,\n",
    "    lr=lr)\n",
    "\n",
    "params_for_os_optimizer = \\\n",
    "        list(decoder_1_b.parameters()) + \\\n",
    "        list(decoder_2_b.parameters())\n",
    "\n",
    "os_optimizer = optim.Adam(\n",
    "    params_for_os_optimizer,\n",
    "    lr=lr)\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "max_eval_TNR = torch.zeros(1, device=device)\n",
    "epochs_without_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52732bd3-fde9-43ef-a087-fd4ddb11527d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(processor_1)\n",
    "wandb.watch(encoder)\n",
    "wandb.watch(decoder_1_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3eeb4-edb8-499a-a8ad-c24bf70f417e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train (from scratch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52b0b122-cf1c-4379-b336-11d9da649979",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046c6c8510f347908f779c77b0389574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving models at epoch 0\n",
      "saving models at epoch 1\n",
      "saving models at epoch 2\n",
      "saving models at epoch 4\n",
      "saving models at epoch 9\n",
      "Early stopping at episode 1499\n",
      "max_eval_TNR: 0.773594981431961\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc7118243984596bda979e136888292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='24.762 MB of 24.811 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.9980…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>CS1 accuracy_eval: </td><td>▇▃█▆▇██▅▅██▁██▃</td></tr><tr><td>CS1 accuracy_train: </td><td>▂▄▆▆▆▆▆▆▆▇▆▆█▆▇▇▇█▇▇▁▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇</td></tr><tr><td>CS2 accuracy_eval: </td><td>▂▆▃██▅▆▇▂▁▆▅█▃▁</td></tr><tr><td>CS2 accuracy_train: </td><td>▁▄▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇█▇▇▅▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇█▇</td></tr><tr><td>Early stopping at episode</td><td>▁</td></tr><tr><td>OS1 Bal. accuracy_eval: </td><td>▁▂▁▂▄▂▂▅▄▁▆▄█▅▅</td></tr><tr><td>OS1 Bal. accuracy_train: </td><td>▁▁▁▁▁▁▁▁▁▃▁▂▂▂▂▂▃▄▃▄▃▄▄▅▅▆▅▅▅▅▅▆▅▄▆▆▆▆█▆</td></tr><tr><td>OS1 accuracy_eval: </td><td>▁▂▁▂▄▂▂▅▅▁▆▅█▆▅</td></tr><tr><td>OS1 accuracy_train: </td><td>▁▁▁▁▁▁▁▁▁▃▁▂▂▂▂▂▃▄▃▄▃▄▄▄▅▆▅▅▅▅▅▆▅▄▆▆▆▆█▆</td></tr><tr><td>OS2 Bal. accuracy_eval: </td><td>▁▆█▂▂▇▁▂▂▄▂▂▇█▂</td></tr><tr><td>OS2 Bal. accuracy_train: </td><td>▁▁▁▂▁▃▃▄▄█▆▆▄▆▆▆▆▇▇▇▆▇▇▇█▆▇▇▇▇▇▇▇▇▇█▇▇▇▇</td></tr><tr><td>OS2 accuracy_eval: </td><td>▃▄█▅▅▇▁▅▄▂▄▅▇█▅</td></tr><tr><td>OS2 accuracy_train: </td><td>▇▇▆▂▁▃▃▄▅█▅▆▃▆▆▆▆▇▇▇▆▇▇▇▇▆▇▇▇▇▇▇▇▇▇█▇▇▇▇</td></tr><tr><td>epoch: </td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>mean dec_1_loss_a_eval: </td><td>▆▅▅▄▆▇█▆▆█▆▇▁▁█</td></tr><tr><td>mean dec_1_loss_a_train: </td><td>▇▇█▇▅▇▇▇▇▅▇▇▆▇▇▆▆▆▆▆█▆▆▆▆▄▅▅▅▅▅▅▅▅▅▅▅▅▁▅</td></tr><tr><td>mean dec_1_loss_b_eval: </td><td>█▇▇▇▆▆█▅▇▇▄▆▁▅█</td></tr><tr><td>mean dec_1_loss_b_train: </td><td>██████▇▇▇▆▇▇▇▆▆▆▆▅▆▅▆▅▅▅▅▃▄▄▄▄▄▃▄▅▃▃▃▃▁▂</td></tr><tr><td>mean dec_2_loss_a_eval: </td><td>▇▆▂▂▅▃▇▆▃█▇▆▁▁▇</td></tr><tr><td>mean dec_2_loss_a_train: </td><td>█▄▃▃▃▃▂▂▃▁▂▃▃▂▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂</td></tr><tr><td>mean dec_2_loss_b_eval: </td><td>▆▆▃▆▇▄▆▇█▆▅▆▂▁█</td></tr><tr><td>mean dec_2_loss_b_train: </td><td>█████▇▇▇▇▆▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▂▃▃▂▂▁▂</td></tr><tr><td>mean proc_reg_loss1 eval: </td><td>▆▁▁▂▄▃▂█▄▄▄▅▁▃▇</td></tr><tr><td>mean proc_reg_loss1 train: </td><td>   ▄▆▄  ▄▃▄▄▄▃▄▃▃▂▃▃█▃▃▃▃▂▃▃▃▃▃▃▃▃▃▃▃▃▁▃</td></tr><tr><td>mean proc_reg_loss2 eval: </td><td>▅▃▁▂▄▃▂█▃▃▄▄▂▂█</td></tr><tr><td>mean proc_reg_loss2 train: </td><td>█  ▂▃▂▂▂▂▂ ▂▂▂▂▂▂▁▂▂▄▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▁▂</td></tr><tr><td>step: </td><td>▁▁▁▂▂▂▁▂▂▃▃▃▃▃▃▄▄▁▄▄▄▅▁▅▅▅▆▆▆▆▆▇▇▁▇▇▇█▂█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>CS1 accuracy_eval: </td><td>0.77778</td></tr><tr><td>CS1 accuracy_train: </td><td>0.9642</td></tr><tr><td>CS2 accuracy_eval: </td><td>0.63333</td></tr><tr><td>CS2 accuracy_train: </td><td>0.95033</td></tr><tr><td>Early stopping at episode</td><td>1499</td></tr><tr><td>OS1 Bal. accuracy_eval: </td><td>0.725</td></tr><tr><td>OS1 Bal. accuracy_train: </td><td>0.84675</td></tr><tr><td>OS1 accuracy_eval: </td><td>0.74118</td></tr><tr><td>OS1 accuracy_train: </td><td>0.84002</td></tr><tr><td>OS2 Bal. accuracy_eval: </td><td>0.5</td></tr><tr><td>OS2 Bal. accuracy_train: </td><td>0.81812</td></tr><tr><td>OS2 accuracy_eval: </td><td>0.75</td></tr><tr><td>OS2 accuracy_train: </td><td>0.72288</td></tr><tr><td>epoch: </td><td>29</td></tr><tr><td>mean dec_1_loss_a_eval: </td><td>1.02774</td></tr><tr><td>mean dec_1_loss_a_train: </td><td>0.96051</td></tr><tr><td>mean dec_1_loss_b_eval: </td><td>1.07102</td></tr><tr><td>mean dec_1_loss_b_train: </td><td>0.89927</td></tr><tr><td>mean dec_2_loss_a_eval: </td><td>1.28023</td></tr><tr><td>mean dec_2_loss_a_train: </td><td>1.33897</td></tr><tr><td>mean dec_2_loss_b_eval: </td><td>1.39031</td></tr><tr><td>mean dec_2_loss_b_train: </td><td>1.2392</td></tr><tr><td>mean proc_reg_loss1 eval: </td><td>81.67643</td></tr><tr><td>mean proc_reg_loss1 train: </td><td>64.17842</td></tr><tr><td>mean proc_reg_loss2 eval: </td><td>88.95996</td></tr><tr><td>mean proc_reg_loss2 train: </td><td>63.30192</td></tr><tr><td>step: </td><td>14900</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">UNSW from scratch</strong> at: <a href='https://wandb.ai/jfcevallos/Nero_1.1/runs/pn5zf7ph' target=\"_blank\">https://wandb.ai/jfcevallos/Nero_1.1/runs/pn5zf7ph</a><br/>Synced 7 W&B file(s), 480 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231222_142033-pn5zf7ph/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    # TRAIN\n",
    "    encoder.train()\n",
    "    processor_1.train()\n",
    "    decoder_1_b.train()\n",
    "    processor_2.train()\n",
    "    decoder_2_b.train()\n",
    "\n",
    "    # reset conf Mats\n",
    "    cs_cm_1 = torch.zeros(\n",
    "        [max_prototype_buffer_micro, max_prototype_buffer_micro],\n",
    "        device=device)\n",
    "    os_cm_1 = torch.zeros([2, 2], device=device)\n",
    "    cs_cm_2 = torch.zeros(\n",
    "        [max_prototype_buffer_macro, max_prototype_buffer_macro],\n",
    "        device=device)\n",
    "    os_cm_2 = torch.zeros([2, 2], device=device)\n",
    "\n",
    "    # reset metrics dict\n",
    "    metrics_dict = utils.reset_metrics_dict()\n",
    "\n",
    "    # go!\n",
    "    for batch_idx, sample_batch in enumerate(train_loader):\n",
    "        # go to cuda:\n",
    "        sample_batch = sample_batch[0].to(device), sample_batch[1].to(device)\n",
    "\n",
    "        # PHASE 1\n",
    "        proc_1_loss, \\\n",
    "            os_1_loss, \\\n",
    "            hiddens_1, \\\n",
    "            decoded_1 = first_phase_simple(\n",
    "                                sample_batch)\n",
    "\n",
    "        # PHASE 2\n",
    "        proc_2_loss, \\\n",
    "            os_2_loss, \\\n",
    "            hiddens_2, \\\n",
    "            decoded_2 = second_phase_simple(\n",
    "                sample_batch,\n",
    "                hiddens_1)\n",
    "\n",
    "        # Learning\n",
    "        proc_loss = proc_1_loss + proc_2_loss\n",
    "        processor_optimizer.zero_grad()\n",
    "        proc_loss.backward()\n",
    "        processor_optimizer.step()\n",
    "\n",
    "        os_loss = os_1_loss + os_2_loss\n",
    "        os_optimizer.zero_grad()\n",
    "        os_loss.backward()\n",
    "        os_optimizer.step()\n",
    "\n",
    "        # Reporting\n",
    "        step = batch_idx + (epoch * train_loader.sampler.n_tasks)\n",
    "\n",
    "        if step % report_step_frequency == 0:\n",
    "            utils.reporting_simple(\n",
    "                'train',\n",
    "                epoch,\n",
    "                metrics_dict,\n",
    "                step,\n",
    "                wb,\n",
    "                wandb)\n",
    "\n",
    "    pu.super_plotting_function(\n",
    "                phase='Training',\n",
    "                labels=sample_batch[1].cpu(),\n",
    "                hiddens_1=hiddens_1.detach().cpu(),\n",
    "                hiddens_2=hiddens_2.detach().cpu(),\n",
    "                scores_1=decoded_1.detach().cpu(),\n",
    "                scores_2=decoded_2.detach().cpu(),\n",
    "                cs_cm_1=cs_cm_1.cpu(),\n",
    "                cs_cm_2=cs_cm_2.cpu(),\n",
    "                os_cm_1=os_cm_1.cpu(),\n",
    "                os_cm_2=os_cm_2.cpu(),\n",
    "                wb=wb,\n",
    "                wandb=wandb,\n",
    "                complete_micro_classes=micro_classes,\n",
    "                complete_macro_classes=macro_classes\n",
    "                )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        # Evaluation\n",
    "        encoder.eval()\n",
    "        processor_1.eval()\n",
    "        decoder_1_b.eval()\n",
    "        processor_2.eval()\n",
    "        decoder_2_b.eval()\n",
    "\n",
    "        # reset conf Mats\n",
    "        cs_cm_1 = torch.zeros(\n",
    "            [max_prototype_buffer_micro, max_prototype_buffer_micro],\n",
    "            device=device)\n",
    "        os_cm_1 = torch.zeros([2, 2], device=device)\n",
    "        cs_cm_2 = torch.zeros(\n",
    "            [max_prototype_buffer_macro, max_prototype_buffer_macro],\n",
    "            device=device)\n",
    "        os_cm_2 = torch.zeros([2, 2], device=device)\n",
    "\n",
    "        # reset metrics dict\n",
    "        metrics_dict = utils.reset_metrics_dict()\n",
    "\n",
    "        # go!\n",
    "        for batch_idx, sample_batch in enumerate(test_loader):\n",
    "            # go to cuda:\n",
    "            sample_batch = sample_batch[0].to(device), sample_batch[1].to(device)\n",
    "\n",
    "            # PHASE 1\n",
    "            proc_1_loss, \\\n",
    "                os_1_loss, \\\n",
    "                hiddens_1, \\\n",
    "                decoded_1 = first_phase_simple(\n",
    "                                    sample_batch)\n",
    "\n",
    "            # PHASE 2\n",
    "            proc_2_loss, \\\n",
    "                os_2_loss, \\\n",
    "                hiddens_2, \\\n",
    "                decoded_2 = second_phase_simple(\n",
    "                    sample_batch,\n",
    "                    hiddens_1)\n",
    "\n",
    "            # Reporting\n",
    "            step = batch_idx + (epoch * test_loader.sampler.n_tasks)\n",
    "\n",
    "            if step % report_step_frequency == 0:\n",
    "                utils.reporting_simple(\n",
    "                    'eval',\n",
    "                    epoch,\n",
    "                    metrics_dict,\n",
    "                    step,\n",
    "                    wb,\n",
    "                    wandb)\n",
    "\n",
    "        pu.super_plotting_function(\n",
    "                phase='Evaluation',\n",
    "                labels=sample_batch[1].cpu(),\n",
    "                hiddens_1=hiddens_1.detach().cpu(),\n",
    "                hiddens_2=hiddens_2.detach().cpu(),\n",
    "                scores_1=decoded_1.detach().cpu(),\n",
    "                scores_2=decoded_2.detach().cpu(),\n",
    "                cs_cm_1=cs_cm_1.cpu(),\n",
    "                cs_cm_2=cs_cm_2.cpu(),\n",
    "                os_cm_1=os_cm_1.cpu(),\n",
    "                os_cm_2=os_cm_2.cpu(),\n",
    "                wb=wb,\n",
    "                wandb=wandb,\n",
    "                complete_micro_classes=micro_classes,\n",
    "                complete_macro_classes=macro_classes\n",
    "            )\n",
    "\n",
    "        # Checking for improvement\n",
    "        curr_TNR = np.array(metrics_dict['OS_2_B_accuracies']).mean()\n",
    "\n",
    "        if curr_TNR > max_eval_TNR:\n",
    "            max_eval_TNR = curr_TNR\n",
    "            epochs_without_improvement = 0\n",
    "            print(f'saving models at epoch {epoch}')\n",
    "            save_stuff(run_name)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping at episode {step}')\n",
    "            if wb:\n",
    "                wandb.log({'Early stopping at episode': step})\n",
    "            break\n",
    "\n",
    "print(f'max_eval_TNR: {max_eval_TNR}')\n",
    "\n",
    "if wb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9831d3-5748-4ac4-9240-2079d5b83c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa5175f-df7d-4cb8-a0e6-40994c8f5b9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train (fine tune a pre-trained Neural algorithmic processor):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7c779-c60a-4e25-8417-89af52749702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor_1.load_state_dict(torch.load('GENNARO-processor.pt'))\n",
    "processor_1.eval()\n",
    "\n",
    "# Freeze the pre-trained processor\n",
    "for param in processor_1.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3ddaf9-58b7-4a27-ad2b-0b0760d91adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    # TRAIN\n",
    "    encoder.train()\n",
    "    decoder_1_b.train()\n",
    "\n",
    "    # reset conf Mats\n",
    "    cs_cm_1 = torch.zeros(\n",
    "        [max_prototype_buffer, max_prototype_buffer],\n",
    "        device=device)\n",
    "    os_cm_1 = torch.zeros([2, 2], device=device)\n",
    "\n",
    "    # reset metrics dict\n",
    "    metrics_dict = utils.reset_metrics_dict()\n",
    "\n",
    "    # go!\n",
    "    for batch_idx, sample_batch in enumerate(train_loader):\n",
    "        # go to cuda:\n",
    "        sample_batch = sample_batch[0].to(device), sample_batch[1].to(device)\n",
    "\n",
    "        # PHASE 1\n",
    "        proc_loss, \\\n",
    "            os_loss, \\\n",
    "            hiddens_1, \\\n",
    "            decoded_1 = first_phase_simple(\n",
    "                                sample_batch)\n",
    "\n",
    "        # Learning\n",
    "        processor_optimizer.zero_grad()\n",
    "        proc_loss.backward()\n",
    "        processor_optimizer.step()\n",
    "\n",
    "        os_loss = os_loss\n",
    "        os_optimizer.zero_grad()\n",
    "        os_loss.backward()\n",
    "        os_optimizer.step()\n",
    "\n",
    "        # Reporting\n",
    "        step = batch_idx + (epoch * len(train_loader))\n",
    "\n",
    "        if step % report_step_frequency == 0:\n",
    "            utils.reporting_gennaro(\n",
    "                'train',\n",
    "                epoch,\n",
    "                metrics_dict,\n",
    "                step,\n",
    "                wb,\n",
    "                wandb)\n",
    "\n",
    "    pu.super_plotting_function_gennaro(\n",
    "                phase='Training',\n",
    "                labels=sample_batch[1].cpu(),\n",
    "                hiddens_1=hiddens_1.detach().cpu(),\n",
    "                scores_1=decoded_1.detach().cpu(),\n",
    "                cs_cm_1=cs_cm_1.cpu(),\n",
    "                os_cm_1=os_cm_1.cpu(),\n",
    "                wb=wb,\n",
    "                wandb=wandb,\n",
    "                complete_classes=classes,\n",
    "                )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        # Evaluation\n",
    "        encoder.eval()\n",
    "        decoder_1_b.eval()\n",
    "\n",
    "        # reset conf Mats\n",
    "        cs_cm_1 = torch.zeros(\n",
    "            [max_prototype_buffer, max_prototype_buffer],\n",
    "            device=device)\n",
    "        os_cm_1 = torch.zeros([2, 2], device=device)\n",
    "\n",
    "        # reset metrics dict\n",
    "        metrics_dict = utils.reset_metrics_dict()\n",
    "\n",
    "        # go!\n",
    "        for batch_idx, sample_batch in enumerate(test_loader):\n",
    "            # go to cuda:\n",
    "            sample_batch = sample_batch[0].to(device), sample_batch[1].to(device)\n",
    "\n",
    "            # PHASE 1\n",
    "            proc_1_loss, \\\n",
    "                os_1_loss, \\\n",
    "                hiddens_1, \\\n",
    "                decoded_1 = first_phase_simple(\n",
    "                                    sample_batch)\n",
    "\n",
    "            # Reporting\n",
    "            step = batch_idx + (epoch * len(test_loader))\n",
    "\n",
    "            if step % report_step_frequency == 0:\n",
    "                utils.reporting_gennaro(\n",
    "                    'eval',\n",
    "                    epoch,\n",
    "                    metrics_dict,\n",
    "                    step,\n",
    "                    wb,\n",
    "                    wandb)\n",
    "\n",
    "        pu.super_plotting_function_gennaro(\n",
    "                phase='Evaluation',\n",
    "                labels=sample_batch[1].cpu(),\n",
    "                hiddens_1=hiddens_1.detach().cpu(),\n",
    "                scores_1=decoded_1.detach().cpu(),\n",
    "                cs_cm_1=cs_cm_1.cpu(),\n",
    "                os_cm_1=os_cm_1.cpu(),\n",
    "                wb=wb,\n",
    "                wandb=wandb,\n",
    "                complete_classes=classes,\n",
    "            )\n",
    "\n",
    "        # Checking for improvement\n",
    "        curr_TNR = utils.get_balanced_accuracy(\n",
    "                pos_labels=sample_batch[1][:, 1].long(),\n",
    "                n_tasks=n_eval_tasks,\n",
    "                os_cm=os_cm_1,\n",
    "                n_w=balanced_acc_n_w\n",
    "                )\n",
    "\n",
    "        if curr_TNR > max_eval_TNR:\n",
    "            max_eval_TNR = curr_TNR\n",
    "            epochs_without_improvement = 0\n",
    "            save_stuff(run_name)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping at episode {step}')\n",
    "            if wb:\n",
    "                wandb.log({'Early stopping at episode': step})\n",
    "            break\n",
    "\n",
    "if wb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7871c-7f9c-4920-9995-e32c65b9b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
