{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca5cd0f1-590d-49ce-9086-7e7ad55f98ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df4bc2-2dc3-49e1-94c7-b40b90325a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from connect import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3607f-0c58-48da-9267-5b8785d7298b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb3b5f-dc7f-4df5-a382-4c6120ee58b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import NAR.utils as utils\n",
    "import NAR.data_management as dm\n",
    "import NAR.plotting_utils as pu\n",
    "import NAR.masking as masking\n",
    "import NAR.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058da714-78cb-42e6-9d69-e38f36ade4ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Reload modules (DEV):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805529e4-73fb-4ac0-9124-d72b08aa7994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reload_modules([utils, dm, pu, masking, models])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711fdca5-ac7a-459a-a840-702f17a24f9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Checkpoint:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a435c133-8228-452c-9675-a163ff954463",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Edge to IIoT set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a91ea2-919c-4ed0-90ea-553941f5bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'data/pre_processed_iiotset.csv',\n",
    "    low_memory=False)\n",
    "df = df[df['Attack_type'] != 'MITM_-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ee6b0-320a-4825-ba67-7844987e0f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop(['Attack_label', 'attack_macro_cat', 'Attack_type'], axis=1)\n",
    "y = df[['attack_macro_cat', 'Attack_type']]\n",
    "\n",
    "categorical_indicator = X.dtypes == 'O'\n",
    "\n",
    "categorical_columns = X.columns[list(np.where(np.array(categorical_indicator)==True)[0])].tolist()\n",
    "cont_columns = list(set(X.columns.tolist()) - set(categorical_columns))\n",
    "\n",
    "cat_idxs = list(np.where(np.array(categorical_indicator)==True)[0])\n",
    "con_idxs = list(set(range(len(X.columns))) - set(cat_idxs))\n",
    "\n",
    "# Categories and target classes to natural numbers:\n",
    "cat_dims = []\n",
    "for col in categorical_columns:\n",
    "    l_enc = LabelEncoder()\n",
    "    X[col] = l_enc.fit_transform(X[col].values)\n",
    "    cat_dims.append(len(l_enc.classes_))\n",
    "\n",
    "X = X.values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "y = y.values\n",
    "macro_l_enc = LabelEncoder()\n",
    "micro_l_enc = LabelEncoder()\n",
    "macro_y = macro_l_enc.fit_transform(y[:, 0])\n",
    "micro_y = micro_l_enc.fit_transform(y[:, 1])\n",
    "\n",
    "# num of classes:\n",
    "num_macro_classes = len(np.unique(macro_y))\n",
    "num_micro_classes = len(np.unique(micro_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b8a3e7-01f3-44b3-802f-f9fdca7f2a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.groupby(['attack_macro_cat', 'Attack_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb49f2-ac00-45fa-8dc8-d9b4dff99f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting to suitable format:\n",
    "data = pd.DataFrame(X)\n",
    "data['Macro Label'] = macro_l_enc.inverse_transform(macro_y)\n",
    "data['Micro Label'] = micro_l_enc.inverse_transform(micro_y)\n",
    "\n",
    "\n",
    "micro_zdas = [\n",
    "        'MITM_0',                       # Type A\n",
    "        'MITM_1',                       # Type A\n",
    "        'MITM_2',                       # Type A\n",
    "        'MITM_3',                       # Type A\n",
    "        'Fingerprinting',              # Type A\n",
    "        'Port_Scanning',               # Type A\n",
    "        'Vulnerability_scanner',       # Type A\n",
    "        'Backdoor',                    # Type B\n",
    "        'DDoS_ICMP',                   # Type B\n",
    "        'DDoS_HTTP',                   # Type B\n",
    "        'SQL_injection',               # Type B\n",
    "        'Uploading',                   # Type B\n",
    "        'Password'                     # Type B\n",
    "        ]\n",
    "\n",
    "micro_type_A_ZdAs = [\n",
    "        'MITM_0',                       # Type A\n",
    "        'MITM_1',                       # Type A\n",
    "        'MITM_2',                       # Type A\n",
    "        'MITM_3',                       # Type A\n",
    "        'Fingerprinting',              # Type A\n",
    "        'Port_Scanning',               # Type A\n",
    "        'Vulnerability_scanner',       # Type A\n",
    "        ]\n",
    "\n",
    "micro_type_B_ZdAs = [\n",
    "        'Backdoor',                    # Type B\n",
    "        'DDoS_ICMP',                   # Type B\n",
    "        'DDoS_HTTP',                   # Type B\n",
    "        'SQL_injection',               # Type B\n",
    "        'Uploading',                   # Type B\n",
    "        'Password'                     # Type B\n",
    "        ]\n",
    "\n",
    "train_type_B_micro_classes = [\n",
    "        'Backdoor',                    # Type B\n",
    "        'DDoS_ICMP',                   # Type B\n",
    "        'Uploading'                    # Type B\n",
    "        ]\n",
    "\n",
    "test_type_B_micro_classes = [\n",
    "        'DDoS_HTTP',                   # Type B\n",
    "        'SQL_injection',               # Type B\n",
    "        'Password'                     # Type B\n",
    "        ]\n",
    "\n",
    "\n",
    "test_type_A_macro_classes = [\n",
    "        'MITM'                         # Type A\n",
    "        ]\n",
    "\n",
    "train_type_A_macro_classes = [\n",
    "        'Information_Gathering'        # Type A\n",
    "        ]\n",
    "\n",
    "\n",
    "data = masking.mask_real_data_lowdim(\n",
    "    data=data,\n",
    "    micro_zdas=micro_zdas,\n",
    "    micro_type_A_ZdAs=micro_type_A_ZdAs,\n",
    "    micro_type_B_ZdAs=micro_type_B_ZdAs\n",
    "    )\n",
    "\n",
    "train_data, test_data = masking.split_real_data(\n",
    "    data,\n",
    "    train_type_B_micro_classes,\n",
    "    test_type_B_micro_classes,\n",
    "    test_type_A_macro_classes,\n",
    "    train_type_A_macro_classes\n",
    "    )\n",
    "\n",
    "micro_classes = data['Micro Label'].unique()\n",
    "macro_classes = data['Macro Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f679d439-c025-4d02-a074-337676244906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/micro_label_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(micro_l_enc, file)\n",
    "\n",
    "with open('data/macro_label_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(macro_l_enc, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a67a36-ecbe-4700-b19a-1043fdccb25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train test split is psuedo-randomic. for this reason we do it just once.\n",
    "train_data.to_csv('data/iiotset_train.csv', index=0)\n",
    "test_data.to_csv('data/iiotset_test.csv', index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87511a44-76b9-40fe-9278-3486ce48008f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Checkpoint 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d32829-cf9c-47a5-b949-39d76892ef4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/iiotset_train.csv', low_memory=False)\n",
    "test_data = pd.read_csv('data/iiotset_test.csv', low_memory=False)\n",
    "\n",
    "data = pd.concat([train_data, test_data])\n",
    "\n",
    "micro_classes = data['Micro Label'].unique()\n",
    "macro_classes = data['Macro Label'].unique()\n",
    "\n",
    "with open('data/micro_label_encoder.pkl', 'rb') as file:\n",
    "    micro_label_encoder = pickle.load(file)\n",
    "\n",
    "with open('data/macro_label_encoder.pkl', 'rb') as file:\n",
    "    macro_label_encoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9a7b3-ab0a-4133-8c27-2fc2bf626c31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b41cf-4e0e-4ff8-a915-ab78ef737cc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## helper code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8e43c-c1a1-4228-8d98-60bb6d378981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_stuff(prefix):\n",
    "    torch.save(\n",
    "        encoder.state_dict(),\n",
    "        prefix+'enc.pt')\n",
    "    torch.save(\n",
    "        decoder_1_b.state_dict(),\n",
    "        prefix+'_dec_b.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc7dc5-62bb-4a98-80fc-a445b34a9c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def first_phase_simple(\n",
    "        sample_batch, batch_idx):\n",
    "\n",
    "    global cs_cm_1\n",
    "    global os_cm_1\n",
    "    global metrics_dict\n",
    "\n",
    "    # get masks: THESE ARE NOT COMPLEMETARY!\n",
    "    zda_mask, \\\n",
    "        known_classes_mask, \\\n",
    "        unknown_1_mask, \\\n",
    "        active_query_mask = utils.get_masks_1(\n",
    "            sample_batch[1],\n",
    "            N_QUERY,\n",
    "            device=device)\n",
    "\n",
    "    # get one_hot_labels:\n",
    "    oh_labels = utils.get_oh_labels(\n",
    "        decimal_labels=sample_batch[1][:, 1].long(),\n",
    "        total_classes=max_prototype_buffer_micro,\n",
    "        device=device)\n",
    "\n",
    "    # mask labels:\n",
    "    oh_masked_labels = utils.get_one_hot_masked_labels(\n",
    "        oh_labels,\n",
    "        unknown_1_mask,\n",
    "        device=device)\n",
    "\n",
    "    # encoding input space:\n",
    "    encoded_inputs = encoder(\n",
    "        sample_batch[0].float())\n",
    "\n",
    "    # processing\n",
    "    decoded_1, hiddens_1, predicted_kernel = processor_1(\n",
    "        encoded_inputs,\n",
    "        oh_masked_labels)\n",
    "\n",
    "    # semantic kernel:\n",
    "    semantic_kernel = oh_labels @ oh_labels.T\n",
    "    # Processor regularization:\n",
    "    proc_1_reg_loss = utils.get_kernel_kernel_loss(\n",
    "        semantic_kernel,\n",
    "        predicted_kernel,\n",
    "        a_w=attr_w,\n",
    "        r_w=rep_w)\n",
    "\n",
    "    # Transform lables for Few_shot Closed-set classif.\n",
    "    # compatible with the design of models.get_centroids functions,\n",
    "    # wich is called by our GAT processors.\n",
    "    unique_labels, transformed_labels = sample_batch[1][:, 1][active_query_mask].unique(\n",
    "        return_inverse=True)\n",
    "\n",
    "    # closed set classification\n",
    "    dec_1_loss_a = decoder_1a_criterion(\n",
    "        decoded_1[active_query_mask],\n",
    "        transformed_labels)\n",
    "\n",
    "    # Detach closed from open set gradients\n",
    "    input_for_os_dec = decoded_1.detach()\n",
    "    input_for_os_dec.requires_grad = True\n",
    "\n",
    "    # Unknown cluster prediction:\n",
    "    predicted_unknown_1s = decoder_1_b(\n",
    "        scores=input_for_os_dec[unknown_1_mask]\n",
    "        )\n",
    "\n",
    "    # open-set loss:\n",
    "    dec_1_loss_b = decoder_1b_criterion(\n",
    "        predicted_unknown_1s,\n",
    "        zda_mask[unknown_1_mask].float().unsqueeze(-1))\n",
    "\n",
    "    # inverse transform cs preds\n",
    "    it_preds = utils.inverse_transform_preds(\n",
    "        transormed_preds=decoded_1[active_query_mask],\n",
    "        real_labels=unique_labels,\n",
    "        real_class_num=max_prototype_buffer_micro)\n",
    "\n",
    "    #\n",
    "    # REPORTING:\n",
    "    #\n",
    "\n",
    "    # Closed set confusion matrix\n",
    "    cs_cm_1 += utils.efficient_cm(\n",
    "        preds=it_preds.detach(),\n",
    "        targets=sample_batch[1][:, 1][active_query_mask].long())\n",
    "\n",
    "    # Open set confusion matrix\n",
    "    os_cm_1 += utils.efficient_os_cm(\n",
    "        preds=(predicted_unknown_1s.detach() > 0.5).long(),\n",
    "        targets=zda_mask[unknown_1_mask].long()\n",
    "        )\n",
    "\n",
    "    # accuracies:\n",
    "    CS_acc = utils.get_acc(\n",
    "        logits_preds=it_preds,\n",
    "        oh_labels=oh_labels[active_query_mask])\n",
    "\n",
    "    OS_acc = utils.get_binary_acc(\n",
    "        logits=predicted_unknown_1s.detach(),\n",
    "        labels=zda_mask[unknown_1_mask].float().unsqueeze(-1))\n",
    "\n",
    "    OS_b_acc = utils.get_balanced_accuracy(\n",
    "                os_cm=os_cm_1,\n",
    "                n_w=balanced_acc_n_w\n",
    "                )\n",
    "\n",
    "    # for reporting:\n",
    "    metrics_dict['losses_1a'][batch_idx] = dec_1_loss_a\n",
    "    metrics_dict['proc_reg_loss1'][batch_idx] = proc_1_reg_loss\n",
    "    metrics_dict['CS_accuracies'][batch_idx] = CS_acc\n",
    "    metrics_dict['losses_1b'][batch_idx] = dec_1_loss_b\n",
    "    metrics_dict['OS_accuracies'][batch_idx] = OS_acc\n",
    "    metrics_dict['OS_B_accuracies'][batch_idx] = OS_b_acc\n",
    "\n",
    "    # Processor loss:\n",
    "    proc_1_loss = dec_1_loss_a + proc_1_reg_loss\n",
    "\n",
    "    return proc_1_loss, \\\n",
    "        dec_1_loss_b, \\\n",
    "        hiddens_1, \\\n",
    "        decoded_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088b0c50-763e-45c0-8898-2bffd6e57221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def second_phase_simple(\n",
    "        sample_batch,\n",
    "        hiddens_1,\n",
    "        batch_idx):\n",
    "\n",
    "    global cs_cm_2\n",
    "    global os_cm_2\n",
    "    global metrics_dict\n",
    "\n",
    "    # get masks: THESE ARE NOT COMPLEMETARY!\n",
    "    type_A_mask, known_macro_classes_mask, \\\n",
    "        unknown_2_mask, active_query_mask_2 = utils.get_masks_2(\n",
    "            sample_batch[1],\n",
    "            N_QUERY,\n",
    "            device=device)\n",
    "\n",
    "    # get one_hot_labels:\n",
    "    oh_labels = utils.get_oh_labels(\n",
    "        decimal_labels=sample_batch[1][:, 0].long(),\n",
    "        total_classes=max_prototype_buffer_macro,\n",
    "        device=device)\n",
    "\n",
    "    # mask labels:\n",
    "    oh_masked_labels = utils.get_one_hot_masked_labels(\n",
    "        oh_labels,\n",
    "        unknown_2_mask,\n",
    "        device=device)\n",
    "\n",
    "    decoded_2, hiddens_2, predicted_kernel_2 = processor_2(\n",
    "        hiddens_1,\n",
    "        oh_masked_labels)\n",
    "\n",
    "    # semantic kernel:\n",
    "    semantic_kernel_2 = oh_labels @ oh_labels.T\n",
    "    # Processor regularization:\n",
    "    proc_2_reg_loss = utils.get_kernel_kernel_loss(\n",
    "        semantic_kernel_2,\n",
    "        predicted_kernel_2,\n",
    "        a_w=attr_w,\n",
    "        r_w=rep_w)\n",
    "\n",
    "    unique_macro_labels, transformed_labels_2 = sample_batch[1][:, 0][active_query_mask_2].unique(\n",
    "        return_inverse=True)\n",
    "\n",
    "    # Closed set: should learn to associate type B's to corr. macro cluster.\n",
    "    # geometrical \"break\" in the real-data case. (GRadients 2A)\n",
    "    dec_2_loss_a = decoder_2a_criterion(\n",
    "        decoded_2[active_query_mask_2],\n",
    "        transformed_labels_2)\n",
    "\n",
    "    input_for_os_dec_2 = decoded_2.detach()\n",
    "    input_for_os_dec_2.requires_grad = True\n",
    "\n",
    "    # Unknown cluster prediction:\n",
    "    predicted_unknown_2s = decoder_2_b(\n",
    "        scores=input_for_os_dec_2[unknown_2_mask]\n",
    "        )\n",
    "\n",
    "    # open-set loss:\n",
    "    dec_2_loss_b = decoder_2b_criterion(\n",
    "        predicted_unknown_2s,\n",
    "        type_A_mask[unknown_2_mask].float().unsqueeze(-1))\n",
    "\n",
    "    # inverse transform cs preds\n",
    "    it_preds = utils.inverse_transform_preds(\n",
    "        transormed_preds=decoded_2[active_query_mask_2],\n",
    "        real_labels=unique_macro_labels,\n",
    "        real_class_num=max_prototype_buffer_macro)\n",
    "\n",
    "    # Closed set confusion matrix\n",
    "    cs_cm_2 += utils.efficient_cm(\n",
    "        preds=it_preds.detach(),\n",
    "        targets=sample_batch[1][:, 0][active_query_mask_2].long(),\n",
    "        )\n",
    "\n",
    "    # Open set confusion matrix\n",
    "    os_cm_2 += utils.efficient_os_cm(\n",
    "        preds=(predicted_unknown_2s.detach() > 0.5).long(),\n",
    "        targets=type_A_mask[unknown_2_mask].long()\n",
    "        )\n",
    "\n",
    "    # accuracies:\n",
    "    CS_acc_2 = utils.get_acc(\n",
    "        logits_preds=it_preds,\n",
    "        oh_labels=oh_labels[active_query_mask_2])\n",
    "\n",
    "    OS_acc_2 = utils.get_binary_acc(\n",
    "        logits=predicted_unknown_2s.detach(),\n",
    "        labels=type_A_mask[unknown_2_mask].float().unsqueeze(-1))\n",
    "\n",
    "    OS_2_B_acc = utils.get_balanced_accuracy(\n",
    "                os_cm=os_cm_2,\n",
    "                n_w=balanced_acc_n_w\n",
    "                )\n",
    "\n",
    "    proc_2_loss = dec_2_loss_a + proc_2_reg_loss\n",
    "\n",
    "    # for reporting:\n",
    "    metrics_dict['losses_2a'][batch_idx] = dec_2_loss_a\n",
    "    metrics_dict['proc_reg_loss2'][batch_idx] = proc_2_reg_loss\n",
    "    metrics_dict['losses_2b'][batch_idx] = dec_2_loss_b\n",
    "    metrics_dict['CS_2_accuracies'][batch_idx] = CS_acc_2\n",
    "    metrics_dict['OS_2_accuracies'][batch_idx] = OS_acc_2\n",
    "    metrics_dict['OS_2_B_accuracies'][batch_idx] = OS_2_B_acc\n",
    "\n",
    "    return proc_2_loss, \\\n",
    "        dec_2_loss_b, \\\n",
    "        hiddens_2, \\\n",
    "        decoded_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d118e201-21e9-454c-9837-93f705a9195c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## init data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d63b9-982b-4e04-ad2b-bb0d8271a971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "natural_inputs_dim = 46\n",
    "save = True\n",
    "wb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab974a58-3d4f-4ed8-90ef-433225757430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "torch_seed = 1234\n",
    "torch.manual_seed(torch_seed)\n",
    "\n",
    "#\n",
    "#\n",
    "# Initialize\n",
    "#\n",
    "#\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dataset and Dataloader:\n",
    "train_dataset = dm.RealFewShotDataset_LowDim(\n",
    "    features=train_data.drop(columns=[\n",
    "        'Micro Label',\n",
    "        'Macro Label',\n",
    "        'ZdA',\n",
    "        'Type_A_ZdA',\n",
    "        'Type_B_ZdA']).values,\n",
    "    df=train_data,\n",
    "    micro_label_enc=micro_label_encoder,\n",
    "    macro_label_enc=macro_label_encoder)\n",
    "\n",
    "test_dataset = dm.RealFewShotDataset_LowDim(\n",
    "    features=test_data.drop(columns=[\n",
    "        'Micro Label',\n",
    "        'Macro Label',\n",
    "        'ZdA',\n",
    "        'Type_A_ZdA',\n",
    "        'Type_B_ZdA']).values,\n",
    "    df=test_data,\n",
    "    micro_label_enc=micro_label_encoder,\n",
    "    macro_label_enc=macro_label_encoder)\n",
    "\n",
    "# Number of classes per task :\n",
    "# two of them are ZdAs, one is a type B and the other a type A\n",
    "N_WAY = 4\n",
    "N_SHOT = 5   # Number of samples per class in the support set\n",
    "N_QUERY = 15  # Number of samples per class in the query set\n",
    "\n",
    "n_train_tasks = 500    # For speedy tests, reduce here...\n",
    "n_eval_tasks = 50     # For speedy tests, reduce here...\n",
    "\n",
    "\n",
    "num_of_test_classes = len(test_dataset.micro_classes)\n",
    "num_of_train_classes = len(train_dataset.micro_classes)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    sampler=dm.FewShotSampler(\n",
    "                dataset=train_dataset,\n",
    "                n_tasks=n_train_tasks,\n",
    "                classes_per_it=N_WAY,\n",
    "                k_shot=N_SHOT,\n",
    "                q_shot=N_QUERY),\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    collate_fn=dm.convenient_cf)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    sampler=dm.FewShotSampler(\n",
    "                dataset=test_dataset,\n",
    "                n_tasks=n_eval_tasks,\n",
    "                classes_per_it=N_WAY,\n",
    "                k_shot=N_SHOT,\n",
    "                q_shot=N_QUERY),\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    "    collate_fn=dm.convenient_cf)\n",
    "\n",
    "# reproducibility\n",
    "train_loader.sampler.reset()\n",
    "test_loader.sampler.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1db0a7-93b7-403d-92ec-b9bb22959cad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## init architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd612f-906b-4168-9bb8-bddad2a06cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Hyper params:\n",
    "###\n",
    "max_prototype_buffer_micro = 40\n",
    "max_prototype_buffer_macro = 10\n",
    "lr = 0.001\n",
    "# training parameters\n",
    "n_epochs = 80\n",
    "norm = \"batch\"\n",
    "dropout = 0.1\n",
    "patience = 50\n",
    "lambda_os = 1\n",
    "\n",
    "processor_attention_heads = 8\n",
    "h_dim = 1024\n",
    "report_step_frequency = 100\n",
    "\n",
    "pos_weight_1 = 1\n",
    "pos_weight_2 = 3\n",
    "\n",
    "architectures = 'GATV5 Confidence Dec'\n",
    "balanced_acc_n_w = 0.5\n",
    "attr_w = 1\n",
    "rep_w = 1\n",
    "run_name = f'IIoTSet from scratch'\n",
    "\n",
    "if wb:\n",
    "    wandb.init(project='Nero_1.1',\n",
    "               name=run_name,\n",
    "               config={\"N_SHOT\": N_SHOT,\n",
    "                       \"N_QUERY\": N_QUERY,\n",
    "                       \"N_WAY\": N_WAY,\n",
    "                       \"num_of_test_classes\": num_of_test_classes,\n",
    "                       \"num_of_train_classes\": num_of_train_classes,\n",
    "                       \"train_batch_size\": N_WAY * (N_SHOT + N_QUERY),\n",
    "                       \"len(train_loader)\": train_loader.sampler.n_tasks,\n",
    "                       \"len(test_dataset)\": test_loader.sampler.n_tasks,\n",
    "                       \"max_prototype_buffer_micro\": max_prototype_buffer_micro,\n",
    "                       \"max_prototype_buffer_macro\": max_prototype_buffer_macro,\n",
    "                       \"device\": device,\n",
    "                       \"natural_inputs_dim\": natural_inputs_dim,\n",
    "                       \"h_dim\": h_dim,\n",
    "                       \"lr\": lr,\n",
    "                       \"n_epochs\": n_epochs,\n",
    "                       \"norm\": norm,\n",
    "                       \"dropout\": dropout,\n",
    "                       \"patience\": patience,\n",
    "                       'zdas': data[data.ZdA == True]['Micro Label'].unique(),\n",
    "                       \"lambda_os\": lambda_os,\n",
    "                       \"positive_weight_1\": pos_weight_1,\n",
    "                       \"positive_weight_2\": pos_weight_2,\n",
    "                       \"architectures\": architectures,\n",
    "                       \"balanced_acc_n_w\": balanced_acc_n_w,\n",
    "                       \"attr_w\": attr_w,\n",
    "                       \"rep_w\": rep_w\n",
    "                       })\n",
    "else:\n",
    "    print(run_name)\n",
    "\n",
    "# Encoder\n",
    "encoder = models.Encoder(\n",
    "    in_features=natural_inputs_dim,\n",
    "    out_features=h_dim,\n",
    "    norm=norm,\n",
    "    dropout=dropout,\n",
    "    ).to(device)\n",
    "\n",
    "# First phase:\n",
    "processor_1 = models.GAT_V5_Processor(\n",
    "                h_dim=h_dim,\n",
    "                processor_attention_heads=processor_attention_heads,\n",
    "                dropout=dropout,\n",
    "                device=device\n",
    "                ).to(device)\n",
    "\n",
    "decoder_1a_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "decoder_1_b = models.Confidence_Decoder(\n",
    "                in_dim=N_WAY-2,  # Subtract 1 ZdA\n",
    "                dropout=dropout,\n",
    "                device=device\n",
    "                ).to(device)\n",
    "\n",
    "decoder_1b_criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.Tensor([pos_weight_1])).to(device)\n",
    "\n",
    "\n",
    "# Second phase:\n",
    "processor_2 = models.GAT_V5_Processor(\n",
    "                h_dim=h_dim,\n",
    "                processor_attention_heads=processor_attention_heads,\n",
    "                dropout=dropout,\n",
    "                device=device\n",
    "                ).to(device)\n",
    "\n",
    "decoder_2a_criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "decoder_2_b = models.Confidence_Decoder(\n",
    "                in_dim=N_WAY-1,\n",
    "                dropout=dropout,\n",
    "                device=device\n",
    "                ).to(device)\n",
    "\n",
    "decoder_2b_criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.Tensor([pos_weight_2])).to(device)\n",
    "\n",
    "\n",
    "params_for_processor_optimizer = \\\n",
    "        list(encoder.parameters()) + \\\n",
    "        list(processor_1.parameters()) + \\\n",
    "        list(processor_2.parameters())\n",
    "\n",
    "\n",
    "processor_optimizer = optim.Adam(\n",
    "    params_for_processor_optimizer,\n",
    "    lr=lr)\n",
    "\n",
    "params_for_os_optimizer = \\\n",
    "        list(decoder_1_b.parameters()) + \\\n",
    "        list(decoder_2_b.parameters())\n",
    "\n",
    "os_optimizer = optim.Adam(\n",
    "    params_for_os_optimizer,\n",
    "    lr=lr)\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "max_eval_TNR = torch.zeros(1, device=device)\n",
    "epochs_without_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52732bd3-fde9-43ef-a087-fd4ddb11527d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.watch(processor_1)\n",
    "wandb.watch(encoder)\n",
    "wandb.watch(decoder_1_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3eeb4-edb8-499a-a8ad-c24bf70f417e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train (from scratch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0b122-cf1c-4379-b336-11d9da649979",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    # TRAIN\n",
    "    encoder.train()\n",
    "    processor_1.train()\n",
    "    decoder_1_b.train()\n",
    "    processor_2.train()\n",
    "    decoder_2_b.train()\n",
    "\n",
    "    # reset conf Mats\n",
    "    cs_cm_1 = torch.zeros(\n",
    "        [max_prototype_buffer_micro, max_prototype_buffer_micro],\n",
    "        device=device)\n",
    "    os_cm_1 = torch.zeros([2, 2], device=device)\n",
    "    cs_cm_2 = torch.zeros(\n",
    "        [max_prototype_buffer_macro, max_prototype_buffer_macro],\n",
    "        device=device)\n",
    "    os_cm_2 = torch.zeros([2, 2], device=device)\n",
    "\n",
    "    # reset metrics dict\n",
    "    metrics_dict = utils.reset_metrics_dict_optimized(\n",
    "        train_loader.sampler.n_tasks,\n",
    "        device)\n",
    "\n",
    "    # go!\n",
    "    for batch_idx, sample_batch in enumerate(train_loader):\n",
    "        # go to cuda:\n",
    "        sample_batch = sample_batch[0].to(device), sample_batch[1].to(device)\n",
    "\n",
    "        # PHASE 1\n",
    "        proc_1_loss, \\\n",
    "            os_1_loss, \\\n",
    "            hiddens_1, \\\n",
    "            decoded_1 = first_phase_simple(\n",
    "                                sample_batch,\n",
    "                                batch_idx)\n",
    "\n",
    "        # PHASE 2\n",
    "        proc_2_loss, \\\n",
    "            os_2_loss, \\\n",
    "            hiddens_2, \\\n",
    "            decoded_2 = second_phase_simple(\n",
    "                sample_batch,\n",
    "                hiddens_1,\n",
    "                batch_idx)\n",
    "\n",
    "        # Learning\n",
    "        proc_loss = proc_1_loss + proc_2_loss\n",
    "        processor_optimizer.zero_grad()\n",
    "        proc_loss.backward()\n",
    "        processor_optimizer.step()\n",
    "\n",
    "        os_loss = os_1_loss + os_2_loss\n",
    "        os_optimizer.zero_grad()\n",
    "        os_loss.backward()\n",
    "        os_optimizer.step()\n",
    "\n",
    "        # Reporting\n",
    "        step = batch_idx + (epoch * train_loader.sampler.n_tasks)\n",
    "\n",
    "        if step % report_step_frequency == 0:\n",
    "            utils.reporting_simple_optimized(\n",
    "                'train',\n",
    "                epoch,\n",
    "                metrics_dict,\n",
    "                batch_idx,\n",
    "                report_step_frequency,\n",
    "                wb,\n",
    "                wandb)\n",
    "\n",
    "    pu.super_plotting_function(\n",
    "                phase='Training',\n",
    "                labels=sample_batch[1].cpu(),\n",
    "                hiddens_1=hiddens_1.detach().cpu(),\n",
    "                hiddens_2=hiddens_2.detach().cpu(),\n",
    "                scores_1=decoded_1.detach().cpu(),\n",
    "                scores_2=decoded_2.detach().cpu(),\n",
    "                cs_cm_1=cs_cm_1.cpu(),\n",
    "                cs_cm_2=cs_cm_2.cpu(),\n",
    "                os_cm_1=os_cm_1.cpu(),\n",
    "                os_cm_2=os_cm_2.cpu(),\n",
    "                wb=wb,\n",
    "                wandb=wandb,\n",
    "                complete_micro_classes=micro_classes,\n",
    "                complete_macro_classes=macro_classes\n",
    "                )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        # Evaluation\n",
    "        encoder.eval()\n",
    "        processor_1.eval()\n",
    "        decoder_1_b.eval()\n",
    "        processor_2.eval()\n",
    "        decoder_2_b.eval()\n",
    "\n",
    "        # reset conf Mats\n",
    "        cs_cm_1 = torch.zeros(\n",
    "            [max_prototype_buffer_micro, max_prototype_buffer_micro],\n",
    "            device=device)\n",
    "        os_cm_1 = torch.zeros([2, 2], device=device)\n",
    "        cs_cm_2 = torch.zeros(\n",
    "            [max_prototype_buffer_macro, max_prototype_buffer_macro],\n",
    "            device=device)\n",
    "        os_cm_2 = torch.zeros([2, 2], device=device)\n",
    "\n",
    "        # reset metrics dict\n",
    "        metrics_dict = utils.reset_metrics_dict_optimized(\n",
    "            test_loader.sampler.n_tasks,\n",
    "            device)\n",
    "\n",
    "        # go!\n",
    "        for batch_idx, sample_batch in enumerate(test_loader):\n",
    "            # go to cuda:\n",
    "            sample_batch = sample_batch[0].to(device), sample_batch[1].to(device)\n",
    "\n",
    "            # PHASE 1\n",
    "            proc_1_loss, \\\n",
    "                os_1_loss, \\\n",
    "                hiddens_1, \\\n",
    "                decoded_1 = first_phase_simple(\n",
    "                                    sample_batch,\n",
    "                                    batch_idx)\n",
    "\n",
    "            # PHASE 2\n",
    "            proc_2_loss, \\\n",
    "                os_2_loss, \\\n",
    "                hiddens_2, \\\n",
    "                decoded_2 = second_phase_simple(\n",
    "                    sample_batch,\n",
    "                    hiddens_1,\n",
    "                    batch_idx)\n",
    "\n",
    "            # Reporting\n",
    "            step = batch_idx + (epoch * test_loader.sampler.n_tasks)\n",
    "\n",
    "            if step % report_step_frequency == 0:\n",
    "                utils.reporting_simple_optimized(\n",
    "                    'eval',\n",
    "                    epoch,\n",
    "                    metrics_dict,\n",
    "                    batch_idx,\n",
    "                    report_step_frequency,\n",
    "                    wb,\n",
    "                    wandb)\n",
    "\n",
    "        pu.super_plotting_function(\n",
    "                phase='Evaluation',\n",
    "                labels=sample_batch[1].cpu(),\n",
    "                hiddens_1=hiddens_1.detach().cpu(),\n",
    "                hiddens_2=hiddens_2.detach().cpu(),\n",
    "                scores_1=decoded_1.detach().cpu(),\n",
    "                scores_2=decoded_2.detach().cpu(),\n",
    "                cs_cm_1=cs_cm_1.cpu(),\n",
    "                cs_cm_2=cs_cm_2.cpu(),\n",
    "                os_cm_1=os_cm_1.cpu(),\n",
    "                os_cm_2=os_cm_2.cpu(),\n",
    "                wb=wb,\n",
    "                wandb=wandb,\n",
    "                complete_micro_classes=micro_classes,\n",
    "                complete_macro_classes=macro_classes\n",
    "            )\n",
    "\n",
    "        # Checking for improvement\n",
    "        curr_TNR = metrics_dict['OS_2_B_accuracies'].mean().item()\n",
    "\n",
    "        if curr_TNR > max_eval_TNR:\n",
    "            max_eval_TNR = curr_TNR\n",
    "            epochs_without_improvement = 0\n",
    "            print(f'saving models at epoch {epoch}')\n",
    "            save_stuff(run_name)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping at episode {step}')\n",
    "            if wb:\n",
    "                wandb.log({'Early stopping at episode': step})\n",
    "            break\n",
    "\n",
    "print(f'max_eval_TNR: {max_eval_TNR}')\n",
    "\n",
    "if wb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9831d3-5748-4ac4-9240-2079d5b83c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa5175f-df7d-4cb8-a0e6-40994c8f5b9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train (fine tune a pre-trained Neural algorithmic processor):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b579aa-55ce-4dbe-b16f-3160c1153699",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_1.load_state_dict(torch.load('Pretraining_proc_1.pt'))\n",
    "processor_2.load_state_dict(torch.load('Pretraining_proc_1.pt'))\n",
    "processor_1.eval()\n",
    "processor_2.eval()\n",
    "\n",
    "# Freeze the pre-trained processor\n",
    "for param in processor_1.parameters():\n",
    "    param.requires_grad = False\n",
    "# Freeze the pre-trained processor\n",
    "for param in processor_2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3ddaf9-58b7-4a27-ad2b-0b0760d91adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    # TRAIN\n",
    "    encoder.train()\n",
    "    decoder_1_b.train()\n",
    "    decoder_2_b.train()\n",
    "\n",
    "    # reset conf Mats\n",
    "    cs_cm_1 = torch.zeros(\n",
    "        [max_prototype_buffer_micro, max_prototype_buffer_micro],\n",
    "        device=device)\n",
    "    os_cm_1 = torch.zeros([2, 2], device=device)\n",
    "    cs_cm_2 = torch.zeros(\n",
    "        [max_prototype_buffer_macro, max_prototype_buffer_macro],\n",
    "        device=device)\n",
    "    os_cm_2 = torch.zeros([2, 2], device=device)\n",
    "\n",
    "    # reset metrics dict\n",
    "    metrics_dict = utils.reset_metrics_dict()\n",
    "\n",
    "    # go!\n",
    "    for batch_idx, sample_batch in enumerate(train_loader):\n",
    "        # go to cuda:\n",
    "        sample_batch = sample_batch[0].to(device), sample_batch[1].to(device)\n",
    "\n",
    "        # PHASE 1\n",
    "        proc_1_loss, \\\n",
    "            os_1_loss, \\\n",
    "            hiddens_1, \\\n",
    "            decoded_1 = first_phase_simple(\n",
    "                                sample_batch)\n",
    "\n",
    "        # PHASE 2\n",
    "        proc_2_loss, \\\n",
    "            os_2_loss, \\\n",
    "            hiddens_2, \\\n",
    "            decoded_2 = second_phase_simple(\n",
    "                sample_batch,\n",
    "                hiddens_1)\n",
    "\n",
    "        # Learning\n",
    "        proc_loss = proc_1_loss + proc_2_loss\n",
    "        processor_optimizer.zero_grad()\n",
    "        proc_loss.backward()\n",
    "        processor_optimizer.step()\n",
    "\n",
    "        os_loss = os_1_loss + os_2_loss\n",
    "        os_optimizer.zero_grad()\n",
    "        os_loss.backward()\n",
    "        os_optimizer.step()\n",
    "\n",
    "        # Reporting\n",
    "        step = batch_idx + (epoch * train_loader.sampler.n_tasks)\n",
    "\n",
    "        if step % report_step_frequency == 0:\n",
    "            utils.reporting_simple(\n",
    "                'train',\n",
    "                epoch,\n",
    "                metrics_dict,\n",
    "                step,\n",
    "                wb,\n",
    "                wandb)\n",
    "\n",
    "    pu.super_plotting_function(\n",
    "                phase='Training',\n",
    "                labels=sample_batch[1].cpu(),\n",
    "                hiddens_1=hiddens_1.detach().cpu(),\n",
    "                hiddens_2=hiddens_2.detach().cpu(),\n",
    "                scores_1=decoded_1.detach().cpu(),\n",
    "                scores_2=decoded_2.detach().cpu(),\n",
    "                cs_cm_1=cs_cm_1.cpu(),\n",
    "                cs_cm_2=cs_cm_2.cpu(),\n",
    "                os_cm_1=os_cm_1.cpu(),\n",
    "                os_cm_2=os_cm_2.cpu(),\n",
    "                wb=wb,\n",
    "                wandb=wandb,\n",
    "                complete_micro_classes=micro_classes,\n",
    "                complete_macro_classes=macro_classes\n",
    "                )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        # Evaluation\n",
    "        encoder.eval()\n",
    "        decoder_1_b.eval()\n",
    "        decoder_2_b.eval()\n",
    "\n",
    "        # reset conf Mats\n",
    "        cs_cm_1 = torch.zeros(\n",
    "            [max_prototype_buffer_micro, max_prototype_buffer_micro],\n",
    "            device=device)\n",
    "        os_cm_1 = torch.zeros([2, 2], device=device)\n",
    "        cs_cm_2 = torch.zeros(\n",
    "            [max_prototype_buffer_macro, max_prototype_buffer_macro],\n",
    "            device=device)\n",
    "        os_cm_2 = torch.zeros([2, 2], device=device)\n",
    "\n",
    "        # reset metrics dict\n",
    "        metrics_dict = utils.reset_metrics_dict()\n",
    "\n",
    "        # go!\n",
    "        for batch_idx, sample_batch in enumerate(test_loader):\n",
    "            # go to cuda:\n",
    "            sample_batch = sample_batch[0].to(device), sample_batch[1].to(device)\n",
    "\n",
    "            # PHASE 1\n",
    "            proc_1_loss, \\\n",
    "                os_1_loss, \\\n",
    "                hiddens_1, \\\n",
    "                decoded_1 = first_phase_simple(\n",
    "                                    sample_batch)\n",
    "\n",
    "            # PHASE 2\n",
    "            proc_2_loss, \\\n",
    "                os_2_loss, \\\n",
    "                hiddens_2, \\\n",
    "                decoded_2 = second_phase_simple(\n",
    "                    sample_batch,\n",
    "                    hiddens_1)\n",
    "\n",
    "            # Reporting\n",
    "            step = batch_idx + (epoch * test_loader.sampler.n_tasks)\n",
    "\n",
    "            if step % report_step_frequency == 0:\n",
    "                utils.reporting_simple(\n",
    "                    'eval',\n",
    "                    epoch,\n",
    "                    metrics_dict,\n",
    "                    step,\n",
    "                    wb,\n",
    "                    wandb)\n",
    "\n",
    "        pu.super_plotting_function(\n",
    "                phase='Evaluation',\n",
    "                labels=sample_batch[1].cpu(),\n",
    "                hiddens_1=hiddens_1.detach().cpu(),\n",
    "                hiddens_2=hiddens_2.detach().cpu(),\n",
    "                scores_1=decoded_1.detach().cpu(),\n",
    "                scores_2=decoded_2.detach().cpu(),\n",
    "                cs_cm_1=cs_cm_1.cpu(),\n",
    "                cs_cm_2=cs_cm_2.cpu(),\n",
    "                os_cm_1=os_cm_1.cpu(),\n",
    "                os_cm_2=os_cm_2.cpu(),\n",
    "                wb=wb,\n",
    "                wandb=wandb,\n",
    "                complete_micro_classes=micro_classes,\n",
    "                complete_macro_classes=macro_classes\n",
    "            )\n",
    "\n",
    "        # Checking for improvement\n",
    "        curr_TNR = np.array(metrics_dict['OS_2_B_accuracies']).mean()\n",
    "\n",
    "        if curr_TNR > max_eval_TNR:\n",
    "            max_eval_TNR = curr_TNR\n",
    "            epochs_without_improvement = 0\n",
    "            save_stuff(run_name)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping at episode {step}')\n",
    "            if wb:\n",
    "                wandb.log({'Early stopping at step': step})\n",
    "            break\n",
    "\n",
    "print(f'max_eval_TNR: {max_eval_TNR}')\n",
    "if wb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7871c-7f9c-4920-9995-e32c65b9b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
